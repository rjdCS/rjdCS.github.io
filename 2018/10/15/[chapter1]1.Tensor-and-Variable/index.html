<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Tensor-and-Variable"><meta name="keywords" content="pytorch,ml"><meta name="author" content="JunDa Ren (Richard Ren)"><meta name="copyright" content="JunDa Ren (Richard Ren)"><title>Tensor-and-Variable | RJD's Blog</title><link rel="shortcut icon" href="/r-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.6.0"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#tensor-and-variable"><span class="toc-number">1.</span> <span class="toc-text"> Tensor and Variable</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#把pytorch当numpy使用"><span class="toc-number">2.</span> <span class="toc-text"> 把Pytorch当Numpy使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ndarray-tensor"><span class="toc-number">2.1.</span> <span class="toc-text"> ndarray -&gt; tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor-ndarray"><span class="toc-number">2.2.</span> <span class="toc-text"> tensor -&gt; ndarray</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor-使用gpu加速"><span class="toc-number">2.3.</span> <span class="toc-text"> tensor 使用GPU加速</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#访问tensor的一些属性"><span class="toc-number">2.4.</span> <span class="toc-text"> 访问tensor的一些属性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#得到tensor的尺寸"><span class="toc-number">2.4.1.</span> <span class="toc-text"> 得到tensor的尺寸</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#得到tensor的数据类型"><span class="toc-number">2.4.2.</span> <span class="toc-text"> 得到tensor的数据类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#得到tensor的维度"><span class="toc-number">2.4.3.</span> <span class="toc-text"> 得到tensor的维度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#得到tensor的所有元素个数"><span class="toc-number">2.4.4.</span> <span class="toc-text"> 得到tensor的所有元素个数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensor"><span class="toc-number">3.</span> <span class="toc-text"> Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#unsqueeze和squeeze"><span class="toc-number">3.1.</span> <span class="toc-text"> unsqueeze和squeeze</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transpose"><span class="toc-number">3.2.</span> <span class="toc-text"> transpose</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#view"><span class="toc-number">3.3.</span> <span class="toc-text"> view</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#variable"><span class="toc-number">4.</span> <span class="toc-text"> Variable</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://avatars2.githubusercontent.com/u/8386277?s=460&amp;v=4"></div><div class="author-info__name text-center">JunDa Ren (Richard Ren)</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">17</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">24</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">12</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(http://ww1.sinaimg.cn/large/73c0c3d4gy1ftgcxsy69qj22bc1jknpg.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">RJD's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">Tensor-and-Variable</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-15</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/深度学习入门之pytorch笔记/">深度学习入门之pytorch笔记</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">3,466</span><span class="post-meta__separator">|</span><span>阅读时长: 18 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>参考书籍：《深度学习入门之PyTorch》</p>
<p><img src="https://ws1.sinaimg.cn/large/73c0c3d4gy1fw83kgh174j208c0b4q30.jpg" alt=""></p>
<h2 id="tensor-and-variable"><a class="markdownIt-Anchor" href="#tensor-and-variable"></a> Tensor and Variable</h2>
<ul>
<li>学会像使用Nunpy一样使用PyTorch。</li>
<li>了解PyTorch中的基本元素Tensor和Variable及其操作方式。</li>
</ul>
<h2 id="把pytorch当numpy使用"><a class="markdownIt-Anchor" href="#把pytorch当numpy使用"></a> 把Pytorch当Numpy使用</h2>
<p>在官方的介绍中，我们知道pytorch是一个拥有强力GPU加速的张量和动态构建网络的库，其主要构件是张量，所以我们可以把 PyTorch 当做 NumPy 来用，PyTorch 的很多操作好 NumPy 都是类似的，但是因为其能够在 GPU 上运行，所以有着比 NumPy 快很多倍的速度。</p>
<p><strong>Pytorch ~= 在GPU上运行的NumPy</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个numpy ndarray</span></span><br><span class="line">numpy_tensor = np.random.rand(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">print(numpy_tensor)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.87671834  0.65099693  0.01443204  0.86437767  0.05526555  0.19707037
   0.31980075  0.45569987  0.55084426  0.51854973  0.90724773  0.92637664
   0.35091482  0.35181815  0.54484753  0.04464745  0.75741114  0.06866861
   0.24160603  0.87301608]
 [ 0.69907807  0.58906811  0.55450631  0.08965591  0.96231908  0.55068155
   0.05353622  0.10340924  0.22236449  0.2206694   0.97491519  0.93034726
   0.1255855   0.20829146  0.48174656  0.94121347  0.88924874  0.02007029
   0.51315347  0.70623185]
 [ 0.99717535  0.59565242  0.12563993  0.46555417  0.37103518  0.80168231
   0.40749745  0.40359193  0.22097031  0.24144694  0.33545357  0.90673468
   0.69166527  0.12008446  0.44869024  0.95355092  0.02208238  0.71849542
   0.81211856  0.76745725]
 [ 0.43926797  0.11906668  0.42721559  0.805084    0.77039625  0.53460481
   0.919281    0.82522411  0.04060207  0.05062097  0.56294927  0.34342204
   0.59246882  0.8667215   0.78952876  0.58095771  0.58110872  0.87857542
   0.07323915  0.73358992]
 [ 0.00585817  0.84519543  0.22637113  0.15647122  0.99496609  0.40067062
   0.18232368  0.7935536   0.81602971  0.83014847  0.61545585  0.6036988
   0.43513091  0.66875533  0.32461406  0.8227532   0.56876562  0.50812508
   0.05532475  0.60492122]
 [ 0.00853932  0.39653739  0.54353069  0.01351573  0.72713855  0.54914985
   0.3056223   0.87360997  0.75886067  0.51094897  0.07045433  0.34037603
   0.44240309  0.77872537  0.30380983  0.77772102  0.89415755  0.43017686
   0.63459965  0.0167745 ]
 [ 0.3141114   0.69033955  0.98697756  0.14166204  0.7908459   0.60667874
   0.53455107  0.25905184  0.33573816  0.11714898  0.05254266  0.55995081
   0.39480033  0.53933878  0.90103943  0.1146809   0.34553712  0.4869314
   0.23622239  0.87550152]
 [ 0.58820095  0.00468145  0.2038515   0.80129468  0.03922198  0.53645535
   0.00744151  0.7428848   0.95891507  0.8000217   0.15445523  0.21895479
   0.04768821  0.66458645  0.39015424  0.38678472  0.7988736   0.95082364
   0.43550889  0.66983198]
 [ 0.60431038  0.45524904  0.27734251  0.82979206  0.73589373  0.85188885
   0.62733639  0.98633564  0.74560514  0.90616709  0.68650776  0.45061205
   0.51653414  0.06825787  0.22076101  0.07987247  0.62070153  0.94322896
   0.29381086  0.14345597]
 [ 0.65886036  0.11996374  0.70215922  0.35765405  0.71392396  0.37902828
   0.89533389  0.25852122  0.45938761  0.58284267  0.88994763  0.21593861
   0.40964462  0.62832812  0.96647919  0.03591668  0.65795729  0.70833149
   0.11147782  0.1973635 ]]
</code></pre>
<h3 id="ndarray-tensor"><a class="markdownIt-Anchor" href="#ndarray-tensor"></a> ndarray -&gt; tensor</h3>
<p>我们可以使用下面的两种方式将numpy的ndarray转换到tensor上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">pytorch_tensor1 = torch.Tensor(numpy_tensor)</span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line">pytorch_tensor2 = torch.from_numpy(numpy_tensor)</span><br><span class="line">print(pytorch_tensor1)</span><br><span class="line">print(<span class="string">"**************"</span>)</span><br><span class="line">print(pytorch_tensor2)</span><br></pre></td></tr></table></figure>
<pre><code>Columns 0 to 9 
 0.8767  0.6510  0.0144  0.8644  0.0553  0.1971  0.3198  0.4557  0.5508  0.5185
 0.6991  0.5891  0.5545  0.0897  0.9623  0.5507  0.0535  0.1034  0.2224  0.2207
 0.9972  0.5957  0.1256  0.4656  0.3710  0.8017  0.4075  0.4036  0.2210  0.2414
 0.4393  0.1191  0.4272  0.8051  0.7704  0.5346  0.9193  0.8252  0.0406  0.0506
 0.0059  0.8452  0.2264  0.1565  0.9950  0.4007  0.1823  0.7936  0.8160  0.8301
 0.0085  0.3965  0.5435  0.0135  0.7271  0.5491  0.3056  0.8736  0.7589  0.5109
 0.3141  0.6903  0.9870  0.1417  0.7908  0.6067  0.5346  0.2591  0.3357  0.1171
 0.5882  0.0047  0.2039  0.8013  0.0392  0.5365  0.0074  0.7429  0.9589  0.8000
 0.6043  0.4552  0.2773  0.8298  0.7359  0.8519  0.6273  0.9863  0.7456  0.9062
 0.6589  0.1200  0.7022  0.3577  0.7139  0.3790  0.8953  0.2585  0.4594  0.5828

Columns 10 to 19 
 0.9072  0.9264  0.3509  0.3518  0.5448  0.0446  0.7574  0.0687  0.2416  0.8730
 0.9749  0.9303  0.1256  0.2083  0.4817  0.9412  0.8892  0.0201  0.5132  0.7062
 0.3355  0.9067  0.6917  0.1201  0.4487  0.9536  0.0221  0.7185  0.8121  0.7675
 0.5629  0.3434  0.5925  0.8667  0.7895  0.5810  0.5811  0.8786  0.0732  0.7336
 0.6155  0.6037  0.4351  0.6688  0.3246  0.8228  0.5688  0.5081  0.0553  0.6049
 0.0705  0.3404  0.4424  0.7787  0.3038  0.7777  0.8942  0.4302  0.6346  0.0168
 0.0525  0.5600  0.3948  0.5393  0.9010  0.1147  0.3455  0.4869  0.2362  0.8755
 0.1545  0.2190  0.0477  0.6646  0.3902  0.3868  0.7989  0.9508  0.4355  0.6698
 0.6865  0.4506  0.5165  0.0683  0.2208  0.0799  0.6207  0.9432  0.2938  0.1435
 0.8899  0.2159  0.4096  0.6283  0.9665  0.0359  0.6580  0.7083  0.1115  0.1974
[torch.FloatTensor of size 10x20]

**************


Columns 0 to 9 
 0.8767  0.6510  0.0144  0.8644  0.0553  0.1971  0.3198  0.4557  0.5508  0.5185
 0.6991  0.5891  0.5545  0.0897  0.9623  0.5507  0.0535  0.1034  0.2224  0.2207
 0.9972  0.5957  0.1256  0.4656  0.3710  0.8017  0.4075  0.4036  0.2210  0.2414
 0.4393  0.1191  0.4272  0.8051  0.7704  0.5346  0.9193  0.8252  0.0406  0.0506
 0.0059  0.8452  0.2264  0.1565  0.9950  0.4007  0.1823  0.7936  0.8160  0.8301
 0.0085  0.3965  0.5435  0.0135  0.7271  0.5491  0.3056  0.8736  0.7589  0.5109
 0.3141  0.6903  0.9870  0.1417  0.7908  0.6067  0.5346  0.2591  0.3357  0.1171
 0.5882  0.0047  0.2039  0.8013  0.0392  0.5365  0.0074  0.7429  0.9589  0.8000
 0.6043  0.4552  0.2773  0.8298  0.7359  0.8519  0.6273  0.9863  0.7456  0.9062
 0.6589  0.1200  0.7022  0.3577  0.7139  0.3790  0.8953  0.2585  0.4594  0.5828

Columns 10 to 19 
 0.9072  0.9264  0.3509  0.3518  0.5448  0.0446  0.7574  0.0687  0.2416  0.8730
 0.9749  0.9303  0.1256  0.2083  0.4817  0.9412  0.8892  0.0201  0.5132  0.7062
 0.3355  0.9067  0.6917  0.1201  0.4487  0.9536  0.0221  0.7185  0.8121  0.7675
 0.5629  0.3434  0.5925  0.8667  0.7895  0.5810  0.5811  0.8786  0.0732  0.7336
 0.6155  0.6037  0.4351  0.6688  0.3246  0.8228  0.5688  0.5081  0.0553  0.6049
 0.0705  0.3404  0.4424  0.7787  0.3038  0.7777  0.8942  0.4302  0.6346  0.0168
 0.0525  0.5600  0.3948  0.5393  0.9010  0.1147  0.3455  0.4869  0.2362  0.8755
 0.1545  0.2190  0.0477  0.6646  0.3902  0.3868  0.7989  0.9508  0.4355  0.6698
 0.6865  0.4506  0.5165  0.0683  0.2208  0.0799  0.6207  0.9432  0.2938  0.1435
 0.8899  0.2159  0.4096  0.6283  0.9665  0.0359  0.6580  0.7083  0.1115  0.1974
[torch.DoubleTensor of size 10x20]
</code></pre>
<p>使用上述两种方法进行转换的时候，会直接将Numpy ndarray的数据类型转换为对应的pytorch tensor数据类型</p>
<h3 id="tensor-ndarray"><a class="markdownIt-Anchor" href="#tensor-ndarray"></a> tensor -&gt; ndarray</h3>
<p>同时我们也可以使用下面的方法将pytorch tensor转换为numpy ndarray</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果pytorch tensor 在cpu上</span></span><br><span class="line">numpy_array = pytorch_tensor1.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果pytorch tensor 在gpu上</span></span><br><span class="line">numpy_array = pytorch_tensor2.cpu().numpy()</span><br></pre></td></tr></table></figure>
<p>在gpu上的tensor不能直接转为numpy ndarray，需要先使用.cpu()转移到cpu上</p>
<h3 id="tensor-使用gpu加速"><a class="markdownIt-Anchor" href="#tensor-使用gpu加速"></a> tensor 使用GPU加速</h3>
<p>我们可以使用一下两种方式将tensor放在gpu上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种方式是定义 cuda 数据类型</span></span><br><span class="line">dtype = torch.cuda.FloatTensor <span class="comment"># 定义默认 GPU 的 数据类型</span></span><br><span class="line">gpu_tensor = torch.randn(<span class="number">10</span>, <span class="number">20</span>).type(dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种方式更简单，推荐使用</span></span><br><span class="line">gpu_tensor = torch.randn(<span class="number">10</span>, <span class="number">20</span>).cuda(<span class="number">0</span>) <span class="comment"># 将 tensor 放到第一个 GPU 上</span></span><br><span class="line">gpu_tensor = torch.randn(<span class="number">10</span>, <span class="number">20</span>).cuda(<span class="number">1</span>) <span class="comment"># 将 tensor 放到第二个 GPU 上</span></span><br></pre></td></tr></table></figure>
<p>将tensor 放回cpu</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpu_tensor = gpu_tensor.cpu()</span><br></pre></td></tr></table></figure>
<h3 id="访问tensor的一些属性"><a class="markdownIt-Anchor" href="#访问tensor的一些属性"></a> 访问tensor的一些属性</h3>
<h4 id="得到tensor的尺寸"><a class="markdownIt-Anchor" href="#得到tensor的尺寸"></a> 得到tensor的尺寸</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到tensor的尺寸</span></span><br><span class="line">print(pytorch_tensor1.shape)  <span class="comment"># 注意没有（）</span></span><br><span class="line">print(pytorch_tensor1.size()) <span class="comment"># 注意有（）</span></span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([10, 20])
torch.Size([10, 20])
</code></pre>
<h4 id="得到tensor的数据类型"><a class="markdownIt-Anchor" href="#得到tensor的数据类型"></a> 得到tensor的数据类型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到tensor的数据类型</span></span><br><span class="line">print(pytorch_tensor1.type())</span><br></pre></td></tr></table></figure>
<pre><code>torch.FloatTensor
</code></pre>
<h4 id="得到tensor的维度"><a class="markdownIt-Anchor" href="#得到tensor的维度"></a> 得到tensor的维度</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到tensor的维度</span></span><br><span class="line">print(pytorch_tensor1.dim())</span><br></pre></td></tr></table></figure>
<pre><code>2
</code></pre>
<h4 id="得到tensor的所有元素个数"><a class="markdownIt-Anchor" href="#得到tensor的所有元素个数"></a> 得到tensor的所有元素个数</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到tensor的所有元素个数</span></span><br><span class="line">print(pytorch_tensor1.numel())</span><br></pre></td></tr></table></figure>
<pre><code>200
</code></pre>
<p><strong>练习</strong><br>
查阅以下<a href="http://pytorch.org/docs/0.3.0/tensors.html" target="_blank" rel="noopener">文档</a>了解 tensor 的数据类型，创建一个 float64、大小是 3 x 2、随机初始化的 tensor，将其转化为 numpy 的 ndarray，输出其数据类型</p>
<p>参考输出: float64</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>,<span class="number">2</span>).type(torch.DoubleTensor).numpy()</span><br><span class="line">print(x.dtype)</span><br></pre></td></tr></table></figure>
<pre><code>float64
</code></pre>
<h2 id="tensor"><a class="markdownIt-Anchor" href="#tensor"></a> Tensor</h2>
<p>Tensor的操作与numpy的操作非常相似</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个1矩阵</span></span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(x) <span class="comment"># 这是一个float tensor</span></span><br></pre></td></tr></table></figure>
<pre><code> 1  1
 1  1
[torch.FloatTensor of size 2x2]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看类型</span></span><br><span class="line">print(x.type())</span><br></pre></td></tr></table></figure>
<pre><code>torch.FloatTensor
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 转化类型</span></span><br><span class="line">x = x.long() <span class="comment"># 转化为长整型</span></span><br><span class="line">x = x.type(torch.DoubleTensor) <span class="comment"># 转化为DoubleTensor</span></span><br><span class="line">x = x.float() <span class="comment"># 转化为float</span></span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code> 1  1
 1  1
[torch.FloatTensor of size 2x2]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建4，3的随机矩阵</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code> 0.0856  1.1375 -0.9027
-0.4270 -0.5988  1.7389
 0.5388  0.9909 -1.6148
-1.3109  0.3646  0.2258
[torch.FloatTensor of size 4x3]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 沿着行取最大值</span></span><br><span class="line">max_value, max_idx = torch.max(x, dim=<span class="number">1</span>) <span class="comment"># 1 行， -2 列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每一行的最大值</span></span><br><span class="line">print(max_value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每一行最大值的下标</span></span><br><span class="line">print(max_idx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完整的</span></span><br><span class="line">print(torch.max(x, dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<pre><code> 1.1375
 1.7389
 0.9909
 0.3646
[torch.FloatTensor of size 4]


 1
 2
 1
 1
[torch.LongTensor of size 4]

(
 1.1375
 1.7389
 0.9909
 0.3646
[torch.FloatTensor of size 4]
, 
 1
 2
 1
 1
[torch.LongTensor of size 4]
)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 沿着列取最大值</span></span><br><span class="line">max_value, max_idx = torch.max(x, dim=<span class="number">-2</span>) <span class="comment"># 1 行， -2 列</span></span><br><span class="line">print(max_value)</span><br><span class="line">print(max_idx)</span><br></pre></td></tr></table></figure>
<pre><code> 0.5388
 1.1375
 1.7389
[torch.FloatTensor of size 3]


 2
 0
 1
[torch.LongTensor of size 3]
</code></pre>
<h3 id="unsqueeze和squeeze"><a class="markdownIt-Anchor" href="#unsqueeze和squeeze"></a> unsqueeze和squeeze</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加维度</span></span><br><span class="line">print(x.shape)</span><br><span class="line">x = x.unsqueeze(<span class="number">0</span>) <span class="comment"># 在第一维度4前面增加一个维度1</span></span><br><span class="line">print(x.shape)</span><br><span class="line">x = x.unsqueeze(<span class="number">1</span>) <span class="comment"># 在第一维度1后面增加一个维度1</span></span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([4, 3])
torch.Size([1, 4, 3])
torch.Size([1, 1, 4, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 减少维度</span></span><br><span class="line">print(x.shape)</span><br><span class="line">x = x.squeeze(<span class="number">0</span>) <span class="comment"># 在第一维度1前面减少第一维</span></span><br><span class="line">print(x.shape)</span><br><span class="line">x = x.squeeze() <span class="comment"># 将 tensor 中所有的一维全部都去掉</span></span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([1, 1, 4, 3])
torch.Size([1, 4, 3])
torch.Size([4, 3])
</code></pre>
<h3 id="transpose"><a class="markdownIt-Anchor" href="#transpose"></a> transpose</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用permute和transpose进行维度交换</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># permute</span></span><br><span class="line">x = x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># permute可以重新排列tensor的维度： 4，3，5</span></span><br><span class="line">print(x.shape)</span><br><span class="line"><span class="comment"># transpose</span></span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">2</span>) <span class="comment"># 0和2交换</span></span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([3, 4, 5])
torch.Size([4, 3, 5])
torch.Size([5, 3, 4])
</code></pre>
<h3 id="view"><a class="markdownIt-Anchor" href="#view"></a> view</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 view 对tensor进行reshape # 确保view的维数一致</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line">x = x.view(<span class="number">-1</span>, <span class="number">5</span>) <span class="comment"># 一个二维（ -1 表示自动计算， 5 表示第二维是5） # 第一维3*4=12</span></span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line">x = x.view(<span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line">x = x.view(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>) </span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([3, 4, 5])
torch.Size([12, 5])
torch.Size([3, 20])
torch.Size([1, 2, 3, 10])
</code></pre>
<p>另外，pytorch中大多数的操作都支持 inplace 操作，也就是可以直接对 tensor 进行操作而不需要另外开辟内存空间（另外赋值），方式非常简单，一般都是在操作的符号后面加_，比如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># unsqueeze 进行inplace</span></span><br><span class="line">x.unsqueeze_(<span class="number">0</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># transpose 进行inplace</span></span><br><span class="line">x.transpose_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([3, 3])
torch.Size([1, 3, 3])
torch.Size([3, 1, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加法</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">y = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两个 tensor 求和</span></span><br><span class="line">z = x + y</span><br><span class="line"><span class="comment"># z = torch.add(x, y)</span></span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>
<pre><code>-0.2662 -0.7680 -0.1087  2.5652
-0.1815  0.5647 -1.0015  2.0150
-0.0432 -0.3962  0.3071 -1.9149
[torch.FloatTensor of size 3x4]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加法 inplace</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">y = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两个 tensor 求和</span></span><br><span class="line">x.add_(y)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code> 1.2407  0.1472  1.3081  2.2414
 3.7193  1.2374  0.6187  0.7753
 0.2197  0.2592  1.2381  0.5845
[torch.FloatTensor of size 3x4]
</code></pre>
<p><strong>小练习</strong></p>
<p>访问<a href="http://pytorch.org/docs/0.3.0/tensors.html" target="_blank" rel="noopener">文档</a>了解 tensor 更多的 api，实现下面的要求</p>
<p>创建一个 float32、4 x 4 的全为1的矩阵，将矩阵正中间 2 x 2 的矩阵，全部修改成2</p>
<p>参考输出</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mspace linebreak="newline"></mspace><mo>[</mo><mi>t</mi><mi>o</mi><mi>r</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">.</mi><mi>F</mi><mi>l</mi><mi>o</mi><mi>a</mi><mi>t</mi><mi>T</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>o</mi><mi>r</mi><mtext> </mtext><mi>o</mi><mi>f</mi><mtext> </mtext><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mtext> </mtext><mn>4</mn><mi>x</mi><mn>4</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">\left[
\begin{matrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
1 &amp; 2 &amp; 2 &amp; 1 \\
1 &amp; 2 &amp; 2 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 1
\end{matrix}
\right] \\
[torch.FloatTensor\ of\ size\ 4x4]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.80204em;vertical-align:-2.15003em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6520099999999998em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.80499em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.40599em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.65201em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6520099999999998em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.80499em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.40599em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.65201em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">t</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">c</span><span class="mord mathdefault">h</span><span class="mord">.</span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace"> </span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace"> </span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mord mathdefault">e</span><span class="mspace"> </span><span class="mord">4</span><span class="mord mathdefault">x</span><span class="mord">4</span><span class="mclose">]</span></span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">4</span>, <span class="number">4</span>).float()</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code> 1  1  1  1
 1  1  1  1
 1  1  1  1
 1  1  1  1
[torch.FloatTensor of size 4x4]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(x[<span class="number">1</span>:<span class="number">3</span>, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">x[<span class="number">1</span>:<span class="number">3</span>, <span class="number">1</span>:<span class="number">3</span>] = <span class="number">2</span></span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code> 1  1
 1  1
[torch.FloatTensor of size 2x2]


 1  1  1  1
 1  2  2  1
 1  2  2  1
 1  1  1  1
[torch.FloatTensor of size 4x4]
</code></pre>
<h2 id="variable"><a class="markdownIt-Anchor" href="#variable"></a> Variable</h2>
<p>Tensor 是 PyTorch 中的完美组件，但是构建神经网络还远远不够，我们需要<strong>能够构建计算图的 tensor，这就是 Variable</strong>。</p>
<p><strong>Variable 是对 tensor 的封装</strong>，操作和 tensor 是一样的。</p>
<p>但是每个<strong>Variabel都有三个属性</strong>：</p>
<ul>
<li>Variable 中的 tensor本身 .data (<strong>Variable.data == tensor</strong>)</li>
<li>对应 tensor 的梯度.grad</li>
<li>这个 Variable 是通过什么方式得到的.grad_fn</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入Variable</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br></pre></td></tr></table></figure>
<p>将tensor 转换为 Variable</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x_tensor = torch.randn(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">y_tensor = torch.randn(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 tensor 变成 Variable</span></span><br><span class="line">x = Variable(x_tensor, requires_grad=<span class="keyword">True</span>) <span class="comment"># 默认Variable是不需要求梯度的，所以我们用这个方式申明需要对其进行求梯度</span></span><br><span class="line">y = Variable(y_tensor, requires_grad=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z = torch.sum(x + y)</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>
<pre><code>Variable containing:
-10.9098
[torch.FloatTensor of size 1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取z的tensor数值</span></span><br><span class="line">print(z.data)</span><br><span class="line"><span class="comment"># 获取得到方式</span></span><br><span class="line">print(z.grad_fn)</span><br></pre></td></tr></table></figure>
<pre><code>-10.9098
[torch.FloatTensor of size 1]

&lt;SumBackward0 object at 0x7fa8781ceac8&gt;
</code></pre>
<p>打出了 z 中的 tensor 数值，同时通过grad_fn知道了其是通过 Sum 这种方式得到的</p>
<p>求 x 和 y 的梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方向传播 ，否则不能求梯度</span></span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br><span class="line">print(y.grad)</span><br></pre></td></tr></table></figure>
<pre><code>Variable containing:
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
[torch.FloatTensor of size 10x5]

Variable containing:
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
[torch.FloatTensor of size 10x5]
</code></pre>
<p>通过.grad我们得到了 x 和 y 的梯度，这里我们使用了 PyTorch 提供的自动求导机制，非常方便，下一小节会具体讲自动求导。</p>
<p><strong>小练习</strong></p>
<p>尝试构建一个函数 $y = x^2 $，然后求 x=2 的导数。</p>
<p>参考输出：4</p>
<p>提示：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">y = x^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>的图像如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0.1</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot(<span class="number">2</span>, <span class="number">4</span>, <span class="string">'ro'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p><img src="https://ws1.sinaimg.cn/large/73c0c3d4gy1fw8wnc1orxj20dz09waa5.jpg" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.FloatTensor([<span class="number">2</span>]), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<pre><code>Variable containing:
 4
[torch.FloatTensor of size 1]
</code></pre>
<p>下一节课程我们将会从导数展开，了解 PyTorch 的自动求导机制</p>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a><a class="post-meta__tags" href="/tags/ml/">ml</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/11/01/新开始/"><i class="fa fa-chevron-left">  </i><span>新开始</span></a></div><div class="next-post pull-right"><a href="/2018/10/13/一文入门matplotlib/"><span>一文入门matplotlib</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(http://ww1.sinaimg.cn/large/73c0c3d4gy1ftgcxsy69qj22bc1jknpg.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By JunDa Ren (Richard Ren)</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.0"></script><script src="/js/fancybox.js?version=1.6.0"></script><script src="/js/sidebar.js?version=1.6.0"></script><script src="/js/copy.js?version=1.6.0"></script><script src="/js/fireworks.js?version=1.6.0"></script><script src="/js/transition.js?version=1.6.0"></script><script src="/js/scroll.js?version=1.6.0"></script><script src="/js/head.js?version=1.6.0"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>