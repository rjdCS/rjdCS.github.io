<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="JunDa Ren (Richard Ren)"><meta name="copyright" content="JunDa Ren (Richard Ren)"><title>致我们终将发生的美好。 | RJD's Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.0"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">JunDa Ren (Richard Ren)</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">16</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">24</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">11</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">RJD's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">RJD's Blog</div><div id="site-sub-title">致我们终将发生的美好。</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2019/02/11/评价指标-分类（识别）模型篇/">评价指标--分类（识别）网络篇</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-02-11</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/计算机视觉/">计算机视觉</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/评价指标/">评价指标</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/分类-识别-网络/">分类(识别)网络</a></span><div class="content"><p>参考：Tensorflow速成课程</p>
<h2 id="真与假与混淆矩阵"><a href="#真与假与混淆矩阵" class="headerlink" title="真与假与混淆矩阵"></a>真与假与混淆矩阵</h2><p>“狼来了”故事为例。</p>
<p>定义：</p>
<p>“狼来了”——真（正样本）</p>
<p>“没有狼”——假（负样本）</p>
<h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><p>可以用一个2×2的混淆矩阵，展示“预测狼”可能出现的结果（4种）：</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>TP(真阳性)</td>
<td>FP(假阳性)</td>
</tr>
<tr>
<td>FN(假阴性)</td>
<td>TN(真阴性)</td>
</tr>
</tbody>
</table>
<h3 id="TP"><a href="#TP" class="headerlink" title="TP"></a>TP</h3><ul>
<li>真实情况：受到狼的威胁。</li>
<li>孩子(预测)：“狼来了”。</li>
<li>结果：孩子是英雄。（好结果）</li>
</ul>
<h3 id="FP"><a href="#FP" class="headerlink" title="FP"></a>FP</h3><ul>
<li>真实情况：狼没有来。</li>
<li>孩子(预测)：“狼来了”。</li>
<li>结果：村民非常生气打看小孩一顿。（坏结果）。</li>
</ul>
<h3 id="FN"><a href="#FN" class="headerlink" title="FN"></a>FN</h3><ul>
<li>真实情况：狼来了。</li>
<li>孩子(预测)：“没有狼”。</li>
<li>结果：狼把所有羊都给吃了。（坏结果）</li>
</ul>
<h3 id="TN"><a href="#TN" class="headerlink" title="TN"></a>TN</h3><ul>
<li>真实情况：狼没有来。</li>
<li>孩子(预测)：“没有狼”。</li>
<li>结果：大家都没有事。（好结果）</li>
</ul>
<p>在看一下混淆举证：</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>TP(真阳性)-好结果</td>
<td>FP(假阳性)-坏结果</td>
</tr>
<tr>
<td>FN(假阴性)-坏结果</td>
<td>TN(真阴性)-好结果</td>
</tr>
</tbody>
</table>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>TP(真阳性)：模型<strong>正确</strong>地将正样本预测为正样本。</p>
<p>TN(真阴性)：模型<strong>正确</strong>地将负样本预测为负样本。</p>
<p>FP(假阳性)：模型<strong>错误</strong>地将负样本预测为正样本。</p>
<p>FN(假阴性)：模型<strong>错误</strong>地将正样本预测为负样本。</p>
<h2 id="准确率"><a href="#准确率" class="headerlink" title="准确率"></a>准确率</h2><p>准确率(Accuracy)是一个用于评估分类模型的指标。</p>
<p>换句话说，<strong>准确率是指模型预测正确的结果所占全部结果的比例</strong>。</p>
<p>准确率的定义如下：<br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1g03mtrtd33j20dd01zq2z.jpg" alt=""></p>
<p>对于二分类，也可以根据正负样本按如下方法计算准确率：<br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1g03mw27gq0j20bf02awee.jpg" alt=""></p>
<p>其中：</p>
<p>TP：真阳性 TN：真阴性</p>
<p>FP：假阳性 FN：假阴性</p>
<p>以肿瘤分类为例，计算一下模型的准确率，模型将100个肿瘤分类恶性(正样本)或良性(负样本)：</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>TP(真阳性)</td>
<td>FP(假阳性)</td>
</tr>
<tr>
<td>FN(假阴性)</td>
<td>TN(真阴性)</td>
</tr>
</tbody>
</table>
<p>TP</p>
<ul>
<li>真实情况：恶性。</li>
<li>模型(预测)：恶性。</li>
<li>TP结果数：1</li>
</ul>
<p>FP</p>
<ul>
<li>真实情况：良性</li>
<li>模型(预测)：恶性</li>
<li>FP结果数：1</li>
</ul>
<p>FN</p>
<ul>
<li>真实情况：恶性</li>
<li>模型(预测)：良性</li>
<li>FN结果数：8</li>
</ul>
<p>TN</p>
<ul>
<li>真实情况：良性</li>
<li>模型(预测)：良性</li>
<li>结果数：90</li>
</ul>
<p>准确率计算如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1g03odtj0i3j20iy0233yi.jpg" alt=""><br>准确率是0.91，即91%。也就是说，总共100个样本中91个预测正确。</p>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p><strong><em>我们是不是可以说，我们的模型在识别恶性肿瘤方面表现得非常出色？</em></strong></p>
<p>可以仔细分析一下正负样本，就可以更好地了解我们模型的效果。</p>
<p>100个肿瘤样本中，91个为良性(90个TN+1个FP)，9个为恶性(1个TP+8个FN)。</p>
<p>91个良性肿瘤中，模型正确地将90个(TN)肿瘤识别为良性。不过，在9个恶性肿瘤中，模型仅仅将1个(TP)正确识别出来。<strong>9个恶性肿瘤有8个未被诊断出来</strong>。</p>
<p>如果有一个模型总是预测良性结果，那么这个模型在我们这个样本上进行预测也会实现相同的准确率(91%:91个良性)。也就是说，我们的模型与那些没有预测能力区分恶性与良性的模型差不多。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>当使用<strong>分类不平衡的数据集</strong>(真负样本标签的数量之间存在明显差异)时，只使用准确率一项并不能反映全面情况。</p>
<h2 id="精确率与召回率"><a href="#精确率与召回率" class="headerlink" title="精确率与召回率"></a>精确率与召回率</h2><h3 id="精确率"><a href="#精确率" class="headerlink" title="精确率"></a>精确率</h3><p><strong>精确率</strong>(Precision)指标回答一下问题：</p>
<ul>
<li>在被识别为正样本的样本中，确实是正样本的比例是多少？</li>
</ul>
<p>定义如下：<br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1g03q8pywj3j207c024glg.jpg" alt=""></p>
<ul>
<li>注：如果模型的<strong>预测结果</strong>中没有假阳性，则模型的精确率为1.0</li>
</ul>
<p>上部分分类肿瘤的模型的精确率计算如下：</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>TP(真阳性) : 1</td>
<td>FP(假阳性) : 1</td>
</tr>
<tr>
<td>FN(假阴性) : 8</td>
<td>TN(真阴性) : 90</td>
</tr>
</tbody>
</table>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1g03s0h10jqj20am0270sm.jpg" alt=""><br>该模型的精确率为0.5，也就是说，<strong>模型在预测恶性肿瘤方面的正确率是50%</strong>。</p>
<p><strong>精确率就是被预测为正样本(P,TP+FP)的数据中正确分类(真正是正样本)的数量(TP)所占的百分比。</strong></p>
<h3 id="召回率"><a href="#召回率" class="headerlink" title="召回率"></a>召回率</h3><p><strong>召回率</strong>(recall)指标解决一下问题：</p>
<ul>
<li>在所有正样本中，被模型正确识别出来的比例是多少？</li>
</ul>
<p>定义如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1g03sch6r5nj20620200sk.jpg" alt=""></p>
<ul>
<li>注：如果模型的<strong>预测结果</strong>中没有假阳性，则模型的召回率为1.0</li>
</ul>
<p>上部分分类肿瘤的模型的召回率计算如下：</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>TP(真阳性) : 1</td>
<td>FP(假阳性) : 1</td>
</tr>
<tr>
<td>FN(假阴性) : 8</td>
<td>TN(真阴性) : 90</td>
</tr>
</tbody>
</table>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1g03sfwh97uj20au01o0sm.jpg" alt=""></p>
<p>该模型的召回率是0.11，也就是说，该模型能够正确识别出所有恶性肿瘤的百分比是11%。</p>
<p><strong>召回率就是实际是正样本(TP+FN)中正确分类(TP)的数量所占的百分比。</strong></p>
<h3 id="精确率和召回率：一场拔河比赛"><a href="#精确率和召回率：一场拔河比赛" class="headerlink" title="精确率和召回率：一场拔河比赛"></a>精确率和召回率：一场拔河比赛</h3><p>“要全面评估模型的有效性，必须同时检查精确率和召回率。遗憾的是，精确率和召回率往往是此消彼长的情况。也就是说，提高精确率通常会降低召回率值，反之亦然。”</p>
<h2 id="ROC曲线与曲线下面积"><a href="#ROC曲线与曲线下面积" class="headerlink" title="ROC曲线与曲线下面积"></a>ROC曲线与曲线下面积</h2><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p><a href="https://developers.google.cn/machine-learning/crash-course/classification/roc-and-auc" target="_blank" rel="noopener">https://developers.google.cn/machine-learning/crash-course/classification/roc-and-auc</a></p>
<p>error: Your local changes to the following files would be overwritten by merge:09c09  HEAD -&gt; mastererror: Your local changes to the following files would be overwritten by merge:09c09  HEAD -&gt; mastererror: Your local changes to the following files would be overwritten by merge:09c09  HEAD -&gt; master</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/01/24/判断是否过拟合、欠拟合的一种方法/">判断是否过拟合、欠拟合的一种方法</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-01-24</time><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/深度学习/">深度学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/thricks/">thricks</a></span><div class="content"><p>在跑模型时，如何从loss知道当前模型的效果。</p>
<h2 id="train-loss-不断下降，val-loss不断下降"><a href="#train-loss-不断下降，val-loss不断下降" class="headerlink" title="train loss 不断下降，val loss不断下降"></a>train loss 不断下降，val loss不断下降</h2><p>说明网络仍在正常学习</p>
<h2 id="train-loss-不断下降，val-loss-趋于不变"><a href="#train-loss-不断下降，val-loss-趋于不变" class="headerlink" title="train loss 不断下降，val loss 趋于不变"></a>train loss 不断下降，val loss 趋于不变</h2><p>说明网络可能过拟合</p>
<h2 id="train-loss-趋于不变，val-loss-不断下降"><a href="#train-loss-趋于不变，val-loss-不断下降" class="headerlink" title="train loss 趋于不变，val loss 不断下降"></a>train loss 趋于不变，val loss 不断下降</h2><p>说明数据集100%有问题</p>
<h2 id="train-loss-趋于不变，val-loss-趋于不变"><a href="#train-loss-趋于不变，val-loss-趋于不变" class="headerlink" title="train loss 趋于不变，val loss 趋于不变"></a>train loss 趋于不变，val loss 趋于不变</h2><p>说明学习遇到瓶颈，需要减少学习率或批量数</p>
<h2 id="train-loss-不断上升，val-loss-不断上升"><a href="#train-loss-不断上升，val-loss-不断上升" class="headerlink" title="train loss 不断上升，val loss 不断上升"></a>train loss 不断上升，val loss 不断上升</h2><ul>
<li>说明网络结果设计不当</li>
<li>训练超参数设置不当</li>
<li>数据集问题？</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/01/07/【第一章】基本句型及补语/">【第一章】基本句型及补语</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-01-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/语法俱乐部/">语法俱乐部</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/英语/">英语</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/语法/">语法</a></span><div class="content"><h2 id="五种单句的基本句型"><a href="#五种单句的基本句型" class="headerlink" title="五种单句的基本句型"></a>五种单句的基本句型</h2><table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>S+V</td>
<td>（主语+动词）</td>
</tr>
<tr>
<td>2</td>
<td>S+V+O</td>
<td>（主语+动词+宾语）</td>
</tr>
<tr>
<td>3</td>
<td>S+V+C</td>
<td>（主语+动词+补语）</td>
</tr>
<tr>
<td>4</td>
<td>S+V+O+O</td>
<td>（主语+动词+宾语+宾语）</td>
</tr>
<tr>
<td>5</td>
<td>S+V+O+C</td>
<td>（主语+动词+宾语+补语）</td>
</tr>
</tbody>
</table>
<hr>
<p>基本句型的五种分类，主要是因为有五种特性不同的动词而造成的。在所有的英语动词中，只有解释为“是”的动词是空的（没有意义）。也只有这样的动词才需要补语来补充句子的意思。也就是说，<strong>要了解补语，只需要研究那些解释为“是”的动词就行。</strong></p>
<p>一个完整的句子，必须要表达完成整的意思。这样的话就需要两部分来完成：<strong>主语和动词</strong>。<br><strong>*主语</strong>，是这个句子所描述的对象。<strong>动词</strong>，构成叙述的主要内容。*<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. John Smith died in World War Two.</span><br><span class="line">2. John Smith killed three enemy soldiers.</span><br></pre></td></tr></table></figure></p>
<ul>
<li>例1中，主语+动词（John Smith died）就可以构成一个完整的意思（其他都是可有可无做修饰的）。像died这种动词，<strong>可以独立发生，不牵扯到别的人或者物，这种动词就是“不及物”动词</strong>。</li>
<li>反观，在例2中，主语+动词（John Smith killed）不能构成一个完整的意思（kill需要一个发生的对象，<em>被杀的东西</em>）。也就是说kill这种，不能独立发生，后面通常必须跟着一个宾语（three enemy soldiers）来“接受”这个动作，这种动词就是“及物”动词。</li>
</ul>
<hr>
<p>例2虽然没有表达出完整的意思，但是我们只能认为是不好的句子，并不是完全没有意思（一个叫John的人杀了一个什么）。<br>但是，<strong>一个句子如果缺少补语的话，那么这个句子完全没有意义。</strong><br>在注意一下：<strong>在英语的所有的动词中，只有解释为“是”的动词是完全没有意义的</strong>。一般的动词，不论是及物还是不及物，都担任了句子中最重要的工作。只有解释为“是”的动词，没有叙述能力，只能扮演引导词的角色。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3. John Smith was a soldier.</span><br></pre></td></tr></table></figure></p>
<p>例3，主语+动词（John Smith was），没有任何意义（不知道John Smith的任何信息），叙述的主要内容的在于后面的a soldier，was表示一个引导、连接的作用。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4. John Smith was courageous.</span><br></pre></td></tr></table></figure></p>
<p>be动词后接形容词，be动词不翻译。（约翰·史密斯很勇敢，而不是<del>约翰·史密斯是很勇敢的</del>）<br>解释为“是”的动词没有叙述能力，只能把主语和后面构成叙述的部分连接起来，所有又叫“<strong>连缀动词</strong>”（Linking Verb）。<strong>跟在这种连缀动词后的部分，因为代替了动词所扮演的叙述角色，补足了句子的完整意思，称之为“补语”（Complement）</strong>。</p>
<h2 id="需要补语的动词有哪些？"><a href="#需要补语的动词有哪些？" class="headerlink" title="需要补语的动词有哪些？"></a>需要补语的动词有哪些？</h2><p>be动词是最具代表性的连缀动词，需要补语。并且凡是可以翻译为“是”（各种各样的“是”）的动词都需要补语。<br>如：<br>look 看起来是</p>
<p>seem 似乎是</p>
<p>appear 显得是</p>
<p>sound 听起来是</p>
<p>feel 摸起来是</p>
<p>taste 尝起来是</p>
<p>turn 转变为</p>
<p>prove 证实为</p>
<p>become 成为</p>
<p>make 做为</p>
<p><strong>主语+动词都不能组成一个有意义的句子，就需要后接补语</strong>。<br>如：<br>That dress <u>looks</u> pretty. (那件裙子很好看。)</p>
<p>这就是<strong>主语+动词+补语</strong>（S+V+C）句型。这种补语称为“<strong>主语补语</strong>”。</p>
<h2 id="宾语补语"><a href="#宾语补语" class="headerlink" title="宾语补语"></a>宾语补语</h2><ul>
<li><strong>主语补语</strong>，是用补语说明主语是什么，用“是”串联起来。</li>
<li><strong>宾语补语</strong>（S+V+O+C），则是用补语说明宾语是什么，宾语与补语之间暗示有一个“是”的关系存在。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">I find the dress pretty.</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>the dress与pretty之间加上be动词后，得到The dress is pretty，就是主语补语的形式。这也是检验句子是否是S+V+O+C句型最简单的方法：把宾语和补语拿出来，中间加be动词，看看能不能改成S+V+C。</p>
<h2 id="补语的词性"><a href="#补语的词性" class="headerlink" title="补语的词性"></a>补语的词性</h2><p><strong>名词</strong>和<strong>形容词</strong></p>
<h2 id="没有补语的be动词"><a href="#没有补语的be动词" class="headerlink" title="没有补语的be动词"></a>没有补语的be动词</h2><p>只有当be动词当做是“存在”解释时，be动词后不接补语，因为此时be动词并不是当做连缀动词使用（只有解释为“是”才是连缀动词）。此时，句型就是最简单的S+V形式。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">I think; therefore I am.</span><br><span class="line">我思故我在</span><br><span class="line">To be or not to be, that is the question. </span><br><span class="line">存在或者不存在（生存还是毁灭），这是一个问题</span><br></pre></td></tr></table></figure></p>
<h2 id="有两个宾语的句型"><a href="#有两个宾语的句型" class="headerlink" title="有两个宾语的句型"></a>有两个宾语的句型</h2><p>第五种句型：S+V+O+O中动词后面可以接两个宾语<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">John&apos;s father gave him a dog.</span><br></pre></td></tr></table></figure></p>
<p>与S+V+O+C区分：<br>S+V+O+C中O+C暗含“是（相等）”的意味。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">John&apos;s father called him a dog.</span><br></pre></td></tr></table></figure></p>
<p>him与a dog之间暗含“是（相等）”的关系。所有a dog是him的补语。反观上一句，him是给的对象，a dog是给的东西，不存在相等关系，都是宾语。</p>
<p>【本章完】</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/01/07/e-cubed/">E-cubed</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-01-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/coach-shane/">coach shane</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/英语/">英语</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/口语/">口语</a></span><div class="content"><h3 id="Content"><a href="#Content" class="headerlink" title="Content"></a><strong><em>Content</em></strong></h3><p>[toc]</p>
<h3 id="01-How-are-you-dong"><a href="#01-How-are-you-dong" class="headerlink" title="01 How are you dong?"></a>01 How are you dong?</h3><hr>
<p><strong>pronunciation</strong>：<br>How ya’ doin?<br><strong>dialog</strong>:<br>A: How are you doing?<br>B: Great! How are you doing?<br>A: Not too bad, thanks!<br>B: Take it wasy~</p>
<hr>
<h3 id="02-What-do-you-do-for-living"><a href="#02-What-do-you-do-for-living" class="headerlink" title="02 What do you do for living"></a>02 What do you do for living</h3><hr>
<p><strong>pronunciation</strong>：<br>What de ya do fer living?<br><strong>means</strong>:<br>=What is your job?=How do you make money?=Do you have a job?=Where do you work?=How do you earn money?<br><strong>dialog</strong>:<br>A: What do you do for living?<br>B: I’m a teacher.<br>A: Oh, really? What do you teach?<br>B: I’m a math teacher at Carson College.</p>
<hr>
<h3 id="03-I’m-into…"><a href="#03-I’m-into…" class="headerlink" title="03 I’m into…"></a>03 I’m into…</h3><hr>
<p><strong>pronunciation</strong>：<br>I mean to…<br><strong>means</strong>:<br>=be interested in sth<br><strong>dialog</strong>:<br>A: I can’t believe you’re watching that.<br>B: The Golf Channel? I’m into golf.<br>A: Since when?<br>B: Since college. I used to play every day.<br><strong>补充</strong>:<br>I’m into 用法有点类似 I get into=熟悉，掌握</p>
<hr>
<h3 id="04-My-knee-went-out"><a href="#04-My-knee-went-out" class="headerlink" title="04 My knee went out."></a>04 My knee went out.</h3><hr>
<p><strong>pronunciation</strong>：<br>wen out<br><strong>point</strong>:<br>在美语中有三个强音s,n,l 遇到三个弱音d,t,th时，经常省略弱音。<br><strong>means</strong>:<br>=not working properly<br><strong>dialog</strong>:<br>A: You wanna play some basketball this weekend?<br>B: I’d love to, but my knee went out.<br>A: Ouch! How did that happen?<br>B: When I was playing soccer~</p>
<hr>
<h3 id="05-What’s-up-this-weekend"><a href="#05-What’s-up-this-weekend" class="headerlink" title="05 What’s up this weekend?"></a>05 What’s up this weekend?</h3><hr>
<p><strong>pronunciation</strong>：<br>wassup …<br><strong>means</strong>:<br>=What are you doning this weekend?<br><strong>dialog</strong>:<br>A: What’s up this weekend?<br>B: I’m gonna go hiking.<br>A: Where at?<br>B: Park Canyon. You wanna go?</p>
<hr>
<h3 id="06-I’m-gonna-kick-back"><a href="#06-I’m-gonna-kick-back" class="headerlink" title="06 I’m gonna kick back."></a>06 I’m gonna kick back.</h3><hr>
<p><strong>point</strong>：<br>I’m gonna = I’m going to<br><strong>means</strong>:<br>kick back = relax<br><strong>dialog</strong>:<br>A: You wanna (go?) do something?<br>B: No! I’m gonna kick back.<br>A: You’re so lazy!<br>B: Hey! I had a long week.</p>
<hr>
<h3 id="07-I-slept-in"><a href="#07-I-slept-in" class="headerlink" title="07 I slept in."></a>07 I slept in.</h3><hr>
<p><strong>pronunciation</strong>：<br>I slepin.<br><strong>means</strong>:<br>= to sleep late in the morning without alarm clock. 睡懒觉，与overslept区别<br><strong>dialog</strong>:<br>A: You look refreshed.<br>B: Yes! I slept in. I woke up at 10!<br>A: I wish I could. I’ve got kids.<br>B: That’s why I’ll never marry!</p>
<hr>
<h3 id="08-I-overslept"><a href="#08-I-overslept" class="headerlink" title="08 I overslept."></a>08 I overslept.</h3><hr>
<p><strong>means</strong>:<br>睡过头。<br><strong>dialog</strong>:<br>A: Where have you been?<br>B: I’m sorry, boss. I overslept!<br>A: Again?<br>B: I’m sorry. It won’t happen again!</p>
<hr>
<h3 id="09-I’m-gonna-stock-up-on-water"><a href="#09-I’m-gonna-stock-up-on-water" class="headerlink" title="09 I’m gonna stock up on water."></a>09 I’m gonna stock up on water.</h3><hr>
<p><strong>pronunciation</strong>：<br>…gonna stockonpon water<br><strong>means</strong>:<br>gonna stock up on = going to buy a lot 大量购买，囤积<br><strong>dialog</strong>:<br>A: What are you getting at the store?<br>B: I’m gonna stock up on water.<br>A: What about cookies?<br>B: Those,too!</p>
<hr>
<h3 id="10-I’m-counting-on-you"><a href="#10-I’m-counting-on-you" class="headerlink" title="10 I’m counting on you."></a>10 I’m counting on you.</h3><hr>
<p><strong>pronunciation</strong>：<br>I’m couning on you<br><strong>means</strong>:<br>=I’m relying on you=I’m trusting in you=I’m believing you<br><strong>dialog</strong>:<br>A: Will you help me move this Sunday?<br>B: Sure!<br>A: I’m counting on you~<br>B: Don’t worry! I’ll be there.</p>
<hr>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/01/新开始/">新开始</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-01</time><div class="content"><p>这是一个新的开始。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/15/[chapter1]1.Tensor-and-Variable/">Tensor-and-Variable</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/深度学习入门之pytorch笔记/">深度学习入门之pytorch笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/pytorch/">pytorch</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/ml/">ml</a></span><div class="content"><p>参考书籍：《深度学习入门之PyTorch》</p>
<p><img src="https://ws1.sinaimg.cn/large/73c0c3d4gy1fw83kgh174j208c0b4q30.jpg" alt=""></p>
<h2 id="Tensor-and-Variable"><a href="#Tensor-and-Variable" class="headerlink" title="Tensor and Variable"></a>Tensor and Variable</h2><ul>
<li>学会像使用Nunpy一样使用PyTorch。</li>
<li>了解PyTorch中的基本元素Tensor和Variable及其操作方式。</li>
</ul>
<h2 id="把Pytorch当Numpy使用"><a href="#把Pytorch当Numpy使用" class="headerlink" title="把Pytorch当Numpy使用"></a>把Pytorch当Numpy使用</h2><p>在官方的介绍中，我们知道pytorch是一个拥有强力GPU加速的张量和动态构建网络的库，其主要构件是张量，所以我们可以把 PyTorch 当做 NumPy 来用，PyTorch 的很多操作好 NumPy 都是类似的，但是因为其能够在 GPU 上运行，所以有着比 NumPy 快很多倍的速度。</p>
<p><strong>Pytorch ~= 在GPU上运行的NumPy</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个numpy ndarray</span></span><br><span class="line">numpy_tensor = np.random.rand(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">print(numpy_tensor)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.87671834  0.65099693  0.01443204  0.86437767  0.05526555  0.19707037
   0.31980075  0.45569987  0.55084426  0.51854973  0.90724773  0.92637664
   0.35091482  0.35181815  0.54484753  0.04464745  0.75741114  0.06866861
   0.24160603  0.87301608]
 [ 0.69907807  0.58906811  0.55450631  0.08965591  0.96231908  0.55068155
   0.05353622  0.10340924  0.22236449  0.2206694   0.97491519  0.93034726
   0.1255855   0.20829146  0.48174656  0.94121347  0.88924874  0.02007029
   0.51315347  0.70623185]
 [ 0.99717535  0.59565242  0.12563993  0.46555417  0.37103518  0.80168231
   0.40749745  0.40359193  0.22097031  0.24144694  0.33545357  0.90673468
   0.69166527  0.12008446  0.44869024  0.95355092  0.02208238  0.71849542
   0.81211856  0.76745725]
 [ 0.43926797  0.11906668  0.42721559  0.805084    0.77039625  0.53460481
   0.919281    0.82522411  0.04060207  0.05062097  0.56294927  0.34342204
   0.59246882  0.8667215   0.78952876  0.58095771  0.58110872  0.87857542
   0.07323915  0.73358992]
 [ 0.00585817  0.84519543  0.22637113  0.15647122  0.99496609  0.40067062
   0.18232368  0.7935536   0.81602971  0.83014847  0.61545585  0.6036988
   0.43513091  0.66875533  0.32461406  0.8227532   0.56876562  0.50812508
   0.05532475  0.60492122]
 [ 0.00853932  0.39653739  0.54353069  0.01351573  0.72713855  0.54914985
   0.3056223   0.87360997  0.75886067  0.51094897  0.07045433  0.34037603
   0.44240309  0.77872537  0.30380983  0.77772102  0.89415755  0.43017686
   0.63459965  0.0167745 ]
 [ 0.3141114   0.69033955  0.98697756  0.14166204  0.7908459   0.60667874
   0.53455107  0.25905184  0.33573816  0.11714898  0.05254266  0.55995081
   0.39480033  0.53933878  0.90103943  0.1146809   0.34553712  0.4869314
   0.23622239  0.87550152]
 [ 0.58820095  0.00468145  0.2038515   0.80129468  0.03922198  0.53645535
   0.00744151  0.7428848   0.95891507  0.8000217   0.15445523  0.21895479
   0.04768821  0.66458645  0.39015424  0.38678472  0.7988736   0.95082364
   0.43550889  0.66983198]
 [ 0.60431038  0.45524904  0.27734251  0.82979206  0.73589373  0.85188885
   0.62733639  0.98633564  0.74560514  0.90616709  0.68650776  0.45061205
   0.51653414  0.06825787  0.22076101  0.07987247  0.62070153  0.94322896
   0.29381086  0.14345597]
 [ 0.65886036  0.11996374  0.70215922  0.35765405  0.71392396  0.37902828
   0.89533389  0.25852122  0.45938761  0.58284267  0.88994763  0.21593861
   0.40964462  0.62832812  0.96647919  0.03591668  0.65795729  0.70833149
   0.11147782  0.1973635 ]]
</code></pre><h3 id="ndarray-gt-tensor"><a href="#ndarray-gt-tensor" class="headerlink" title="ndarray -&gt; tensor"></a>ndarray -&gt; tensor</h3><p>我们可以使用下面的两种方式将numpy的ndarray转换到tensor上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">pytorch_tensor1 = torch.Tensor(numpy_tensor)</span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line">pytorch_tensor2 = torch.from_numpy(numpy_tensor)</span><br><span class="line">print(pytorch_tensor1)</span><br><span class="line">print(<span class="string">"**************"</span>)</span><br><span class="line">print(pytorch_tensor2)</span><br></pre></td></tr></table></figure>
<pre><code>Columns 0 to 9 
 0.8767  0.6510  0.0144  0.8644  0.0553  0.1971  0.3198  0.4557  0.5508  0.5185
 0.6991  0.5891  0.5545  0.0897  0.9623  0.5507  0.0535  0.1034  0.2224  0.2207
 0.9972  0.5957  0.1256  0.4656  0.3710  0.8017  0.4075  0.4036  0.2210  0.2414
 0.4393  0.1191  0.4272  0.8051  0.7704  0.5346  0.9193  0.8252  0.0406  0.0506
 0.0059  0.8452  0.2264  0.1565  0.9950  0.4007  0.1823  0.7936  0.8160  0.8301
 0.0085  0.3965  0.5435  0.0135  0.7271  0.5491  0.3056  0.8736  0.7589  0.5109
 0.3141  0.6903  0.9870  0.1417  0.7908  0.6067  0.5346  0.2591  0.3357  0.1171
 0.5882  0.0047  0.2039  0.8013  0.0392  0.5365  0.0074  0.7429  0.9589  0.8000
 0.6043  0.4552  0.2773  0.8298  0.7359  0.8519  0.6273  0.9863  0.7456  0.9062
 0.6589  0.1200  0.7022  0.3577  0.7139  0.3790  0.8953  0.2585  0.4594  0.5828

Columns 10 to 19 
 0.9072  0.9264  0.3509  0.3518  0.5448  0.0446  0.7574  0.0687  0.2416  0.8730
 0.9749  0.9303  0.1256  0.2083  0.4817  0.9412  0.8892  0.0201  0.5132  0.7062
 0.3355  0.9067  0.6917  0.1201  0.4487  0.9536  0.0221  0.7185  0.8121  0.7675
 0.5629  0.3434  0.5925  0.8667  0.7895  0.5810  0.5811  0.8786  0.0732  0.7336
 0.6155  0.6037  0.4351  0.6688  0.3246  0.8228  0.5688  0.5081  0.0553  0.6049
 0.0705  0.3404  0.4424  0.7787  0.3038  0.7777  0.8942  0.4302  0.6346  0.0168
 0.0525  0.5600  0.3948  0.5393  0.9010  0.1147  0.3455  0.4869  0.2362  0.8755
 0.1545  0.2190  0.0477  0.6646  0.3902  0.3868  0.7989  0.9508  0.4355  0.6698
 0.6865  0.4506  0.5165  0.0683  0.2208  0.0799  0.6207  0.9432  0.2938  0.1435
 0.8899  0.2159  0.4096  0.6283  0.9665  0.0359  0.6580  0.7083  0.1115  0.1974
[torch.FloatTensor of size 10x20]

**************


Columns 0 to 9 
 0.8767  0.6510  0.0144  0.8644  0.0553  0.1971  0.3198  0.4557  0.5508  0.5185
 0.6991  0.5891  0.5545  0.0897  0.9623  0.5507  0.0535  0.1034  0.2224  0.2207
 0.9972  0.5957  0.1256  0.4656  0.3710  0.8017  0.4075  0.4036  0.2210  0.2414
 0.4393  0.1191  0.4272  0.8051  0.7704  0.5346  0.9193  0.8252  0.0406  0.0506
 0.0059  0.8452  0.2264  0.1565  0.9950  0.4007  0.1823  0.7936  0.8160  0.8301
 0.0085  0.3965  0.5435  0.0135  0.7271  0.5491  0.3056  0.8736  0.7589  0.5109
 0.3141  0.6903  0.9870  0.1417  0.7908  0.6067  0.5346  0.2591  0.3357  0.1171
 0.5882  0.0047  0.2039  0.8013  0.0392  0.5365  0.0074  0.7429  0.9589  0.8000
 0.6043  0.4552  0.2773  0.8298  0.7359  0.8519  0.6273  0.9863  0.7456  0.9062
 0.6589  0.1200  0.7022  0.3577  0.7139  0.3790  0.8953  0.2585  0.4594  0.5828

Columns 10 to 19 
 0.9072  0.9264  0.3509  0.3518  0.5448  0.0446  0.7574  0.0687  0.2416  0.8730
 0.9749  0.9303  0.1256  0.2083  0.4817  0.9412  0.8892  0.0201  0.5132  0.7062
 0.3355  0.9067  0.6917  0.1201  0.4487  0.9536  0.0221  0.7185  0.8121  0.7675
 0.5629  0.3434  0.5925  0.8667  0.7895  0.5810  0.5811  0.8786  0.0732  0.7336
 0.6155  0.6037  0.4351  0.6688  0.3246  0.8228  0.5688  0.5081  0.0553  0.6049
 0.0705  0.3404  0.4424  0.7787  0.3038  0.7777  0.8942  0.4302  0.6346  0.0168
 0.0525  0.5600  0.3948  0.5393  0.9010  0.1147  0.3455  0.4869  0.2362  0.8755
 0.1545  0.2190  0.0477  0.6646  0.3902  0.3868  0.7989  0.9508  0.4355  0.6698
 0.6865  0.4506  0.5165  0.0683  0.2208  0.0799  0.6207  0.9432  0.2938  0.1435
 0.8899  0.2159  0.4096  0.6283  0.9665  0.0359  0.6580  0.7083  0.1115  0.1974
[torch.DoubleTensor of size 10x20]
</code></pre><p>使用上述两种方法进行转换的时候，会直接将Numpy ndarray的数据类型转换为对应的pytorch tensor数据类型</p>
<h3 id="tensor-gt-ndarray"><a href="#tensor-gt-ndarray" class="headerlink" title="tensor -&gt; ndarray"></a>tensor -&gt; ndarray</h3><p>同时我们也可以使用下面的方法将pytorch tensor转换为numpy ndarray</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果pytorch tensor 在cpu上</span></span><br><span class="line">numpy_array = pytorch_tensor1.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果pytorch tensor 在gpu上</span></span><br><span class="line">numpy_array = pytorch_tensor2.cpu().numpy()</span><br></pre></td></tr></table></figure>
<p>在gpu上的tensor不能直接转为numpy ndarray，需要先使用.cpu()转移到cpu上</p>
<h3 id="tensor-使用GPU加速"><a href="#tensor-使用GPU加速" class="headerlink" title="tensor 使用GPU加速"></a>tensor 使用GPU加速</h3><p>我们可以使用一下两种方式将tensor放在gpu上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种方式是定义 cuda 数据类型</span></span><br><span class="line">dtype = torch.cuda.FloatTensor <span class="comment"># 定义默认 GPU 的 数据类型</span></span><br><span class="line">gpu_tensor = torch.randn(<span class="number">10</span>, <span class="number">20</span>).type(dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种方式更简单，推荐使用</span></span><br><span class="line">gpu_tensor = torch.randn(<span class="number">10</span>, <span class="number">20</span>).cuda(<span class="number">0</span>) <span class="comment"># 将 tensor 放到第一个 GPU 上</span></span><br><span class="line">gpu_tensor = torch.randn(<span class="number">10</span>, <span class="number">20</span>).cuda(<span class="number">1</span>) <span class="comment"># 将 tensor 放到第二个 GPU 上</span></span><br></pre></td></tr></table></figure>
<p>将tensor 放回cpu</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpu_tensor = gpu_tensor.cpu()</span><br></pre></td></tr></table></figure>
<h3 id="访问tensor的一些属性"><a href="#访问tensor的一些属性" class="headerlink" title="访问tensor的一些属性"></a>访问tensor的一些属性</h3><h4 id="得到tensor的尺寸"><a href="#得到tensor的尺寸" class="headerlink" title="得到tensor的尺寸"></a>得到tensor的尺寸</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到tensor的尺寸</span></span><br><span class="line">print(pytorch_tensor1.shape)  <span class="comment"># 注意没有（）</span></span><br><span class="line">print(pytorch_tensor1.size()) <span class="comment"># 注意有（）</span></span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([10, 20])
torch.Size([10, 20])
</code></pre><h4 id="得到tensor的数据类型"><a href="#得到tensor的数据类型" class="headerlink" title="得到tensor的数据类型"></a>得到tensor的数据类型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到tensor的数据类型</span></span><br><span class="line">print(pytorch_tensor1.type())</span><br></pre></td></tr></table></figure>
<pre><code>torch.FloatTensor
</code></pre><h4 id="得到tensor的维度"><a href="#得到tensor的维度" class="headerlink" title="得到tensor的维度"></a>得到tensor的维度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到tensor的维度</span></span><br><span class="line">print(pytorch_tensor1.dim())</span><br></pre></td></tr></table></figure>
<pre><code>2
</code></pre><h4 id="得到tensor的所有元素个数"><a href="#得到tensor的所有元素个数" class="headerlink" title="得到tensor的所有元素个数"></a>得到tensor的所有元素个数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 得到tensor的所有元素个数</span></span><br><span class="line">print(pytorch_tensor1.numel())</span><br></pre></td></tr></table></figure>
<pre><code>200
</code></pre><p><strong>练习</strong><br>查阅以下<a href="http://pytorch.org/docs/0.3.0/tensors.html" target="_blank" rel="noopener">文档</a>了解 tensor 的数据类型，创建一个 float64、大小是 3 x 2、随机初始化的 tensor，将其转化为 numpy 的 ndarray，输出其数据类型</p>
<p>参考输出: float64</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>,<span class="number">2</span>).type(torch.DoubleTensor).numpy()</span><br><span class="line">print(x.dtype)</span><br></pre></td></tr></table></figure>
<pre><code>float64
</code></pre><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>Tensor的操作与numpy的操作非常相似</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个1矩阵</span></span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(x) <span class="comment"># 这是一个float tensor</span></span><br></pre></td></tr></table></figure>
<pre><code> 1  1
 1  1
[torch.FloatTensor of size 2x2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看类型</span></span><br><span class="line">print(x.type())</span><br></pre></td></tr></table></figure>
<pre><code>torch.FloatTensor
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 转化类型</span></span><br><span class="line">x = x.long() <span class="comment"># 转化为长整型</span></span><br><span class="line">x = x.type(torch.DoubleTensor) <span class="comment"># 转化为DoubleTensor</span></span><br><span class="line">x = x.float() <span class="comment"># 转化为float</span></span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code> 1  1
 1  1
[torch.FloatTensor of size 2x2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建4，3的随机矩阵</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code> 0.0856  1.1375 -0.9027
-0.4270 -0.5988  1.7389
 0.5388  0.9909 -1.6148
-1.3109  0.3646  0.2258
[torch.FloatTensor of size 4x3]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 沿着行取最大值</span></span><br><span class="line">max_value, max_idx = torch.max(x, dim=<span class="number">1</span>) <span class="comment"># 1 行， -2 列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每一行的最大值</span></span><br><span class="line">print(max_value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每一行最大值的下标</span></span><br><span class="line">print(max_idx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完整的</span></span><br><span class="line">print(torch.max(x, dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<pre><code> 1.1375
 1.7389
 0.9909
 0.3646
[torch.FloatTensor of size 4]


 1
 2
 1
 1
[torch.LongTensor of size 4]

(
 1.1375
 1.7389
 0.9909
 0.3646
[torch.FloatTensor of size 4]
, 
 1
 2
 1
 1
[torch.LongTensor of size 4]
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 沿着列取最大值</span></span><br><span class="line">max_value, max_idx = torch.max(x, dim=<span class="number">-2</span>) <span class="comment"># 1 行， -2 列</span></span><br><span class="line">print(max_value)</span><br><span class="line">print(max_idx)</span><br></pre></td></tr></table></figure>
<pre><code> 0.5388
 1.1375
 1.7389
[torch.FloatTensor of size 3]


 2
 0
 1
[torch.LongTensor of size 3]
</code></pre><h3 id="unsqueeze和squeeze"><a href="#unsqueeze和squeeze" class="headerlink" title="unsqueeze和squeeze"></a>unsqueeze和squeeze</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加维度</span></span><br><span class="line">print(x.shape)</span><br><span class="line">x = x.unsqueeze(<span class="number">0</span>) <span class="comment"># 在第一维度4前面增加一个维度1</span></span><br><span class="line">print(x.shape)</span><br><span class="line">x = x.unsqueeze(<span class="number">1</span>) <span class="comment"># 在第一维度1后面增加一个维度1</span></span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([4, 3])
torch.Size([1, 4, 3])
torch.Size([1, 1, 4, 3])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 减少维度</span></span><br><span class="line">print(x.shape)</span><br><span class="line">x = x.squeeze(<span class="number">0</span>) <span class="comment"># 在第一维度1前面减少第一维</span></span><br><span class="line">print(x.shape)</span><br><span class="line">x = x.squeeze() <span class="comment"># 将 tensor 中所有的一维全部都去掉</span></span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([1, 1, 4, 3])
torch.Size([1, 4, 3])
torch.Size([4, 3])
</code></pre><h3 id="transpose"><a href="#transpose" class="headerlink" title="transpose"></a>transpose</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用permute和transpose进行维度交换</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># permute</span></span><br><span class="line">x = x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># permute可以重新排列tensor的维度： 4，3，5</span></span><br><span class="line">print(x.shape)</span><br><span class="line"><span class="comment"># transpose</span></span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">2</span>) <span class="comment"># 0和2交换</span></span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([3, 4, 5])
torch.Size([4, 3, 5])
torch.Size([5, 3, 4])
</code></pre><h3 id="view"><a href="#view" class="headerlink" title="view"></a>view</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 view 对tensor进行reshape # 确保view的维数一致</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line">x = x.view(<span class="number">-1</span>, <span class="number">5</span>) <span class="comment"># 一个二维（ -1 表示自动计算， 5 表示第二维是5） # 第一维3*4=12</span></span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line">x = x.view(<span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line">x = x.view(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>) </span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([3, 4, 5])
torch.Size([12, 5])
torch.Size([3, 20])
torch.Size([1, 2, 3, 10])
</code></pre><p>另外，pytorch中大多数的操作都支持 inplace 操作，也就是可以直接对 tensor 进行操作而不需要另外开辟内存空间（另外赋值），方式非常简单，一般都是在操作的符号后面加_，比如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># unsqueeze 进行inplace</span></span><br><span class="line">x.unsqueeze_(<span class="number">0</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># transpose 进行inplace</span></span><br><span class="line">x.transpose_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([3, 3])
torch.Size([1, 3, 3])
torch.Size([3, 1, 3])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加法</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">y = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两个 tensor 求和</span></span><br><span class="line">z = x + y</span><br><span class="line"><span class="comment"># z = torch.add(x, y)</span></span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>
<pre><code>-0.2662 -0.7680 -0.1087  2.5652
-0.1815  0.5647 -1.0015  2.0150
-0.0432 -0.3962  0.3071 -1.9149
[torch.FloatTensor of size 3x4]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加法 inplace</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">y = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两个 tensor 求和</span></span><br><span class="line">x.add_(y)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code> 1.2407  0.1472  1.3081  2.2414
 3.7193  1.2374  0.6187  0.7753
 0.2197  0.2592  1.2381  0.5845
[torch.FloatTensor of size 3x4]
</code></pre><p><strong>小练习</strong></p>
<p>访问<a href="http://pytorch.org/docs/0.3.0/tensors.html" target="_blank" rel="noopener">文档</a>了解 tensor 更多的 api，实现下面的要求</p>
<p>创建一个 float32、4 x 4 的全为1的矩阵，将矩阵正中间 2 x 2 的矩阵，全部修改成2</p>
<p>参考输出</p>
<p>$$<br>\left[<br>\begin{matrix}<br>1 &amp; 1 &amp; 1 &amp; 1 \<br>1 &amp; 2 &amp; 2 &amp; 1 \<br>1 &amp; 2 &amp; 2 &amp; 1 \<br>1 &amp; 1 &amp; 1 &amp; 1<br>\end{matrix}<br>\right] \<br>[torch.FloatTensor\ of\ size\ 4x4]<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">4</span>, <span class="number">4</span>).float()</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code> 1  1  1  1
 1  1  1  1
 1  1  1  1
 1  1  1  1
[torch.FloatTensor of size 4x4]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(x[<span class="number">1</span>:<span class="number">3</span>, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">x[<span class="number">1</span>:<span class="number">3</span>, <span class="number">1</span>:<span class="number">3</span>] = <span class="number">2</span></span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code> 1  1
 1  1
[torch.FloatTensor of size 2x2]


 1  1  1  1
 1  2  2  1
 1  2  2  1
 1  1  1  1
[torch.FloatTensor of size 4x4]
</code></pre><h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><p>Tensor 是 PyTorch 中的完美组件，但是构建神经网络还远远不够，我们需要<strong>能够构建计算图的 tensor，这就是 Variable</strong>。</p>
<p><strong>Variable 是对 tensor 的封装</strong>，操作和 tensor 是一样的。</p>
<p>但是每个<strong>Variabel都有三个属性</strong>：</p>
<ul>
<li>Variable 中的 tensor本身 .data (<strong>Variable.data == tensor</strong>)</li>
<li>对应 tensor 的梯度.grad</li>
<li>这个 Variable 是通过什么方式得到的.grad_fn</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入Variable</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br></pre></td></tr></table></figure>
<p>将tensor 转换为 Variable</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x_tensor = torch.randn(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">y_tensor = torch.randn(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 tensor 变成 Variable</span></span><br><span class="line">x = Variable(x_tensor, requires_grad=<span class="keyword">True</span>) <span class="comment"># 默认Variable是不需要求梯度的，所以我们用这个方式申明需要对其进行求梯度</span></span><br><span class="line">y = Variable(y_tensor, requires_grad=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z = torch.sum(x + y)</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>
<pre><code>Variable containing:
-10.9098
[torch.FloatTensor of size 1]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取z的tensor数值</span></span><br><span class="line">print(z.data)</span><br><span class="line"><span class="comment"># 获取得到方式</span></span><br><span class="line">print(z.grad_fn)</span><br></pre></td></tr></table></figure>
<pre><code>-10.9098
[torch.FloatTensor of size 1]

&lt;SumBackward0 object at 0x7fa8781ceac8&gt;
</code></pre><p>打出了 z 中的 tensor 数值，同时通过grad_fn知道了其是通过 Sum 这种方式得到的</p>
<p>求 x 和 y 的梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方向传播 ，否则不能求梯度</span></span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br><span class="line">print(y.grad)</span><br></pre></td></tr></table></figure>
<pre><code>Variable containing:
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
[torch.FloatTensor of size 10x5]

Variable containing:
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
    1     1     1     1     1
[torch.FloatTensor of size 10x5]
</code></pre><p>通过.grad我们得到了 x 和 y 的梯度，这里我们使用了 PyTorch 提供的自动求导机制，非常方便，下一小节会具体讲自动求导。</p>
<p><strong>小练习</strong></p>
<p>尝试构建一个函数 $y = x^2 $，然后求 x=2 的导数。</p>
<p>参考输出：4</p>
<p>提示：</p>
<p>$y = x^2$的图像如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0.1</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot(<span class="number">2</span>, <span class="number">4</span>, <span class="string">'ro'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p><img src="https://ws1.sinaimg.cn/large/73c0c3d4gy1fw8wnc1orxj20dz09waa5.jpg" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.FloatTensor([<span class="number">2</span>]), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<pre><code>Variable containing:
 4
[torch.FloatTensor of size 1]
</code></pre><p>下一节课程我们将会从导数展开，了解 PyTorch 的自动求导机制</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/13/一文入门matplotlib/">一文入门matplotlib</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-13</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/matplotlib/">matplotlib</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/python/">python</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/numpy/">numpy</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/matplotlib/">matplotlib</a></span><div class="content"><p>Matplotlib是一个作图库。这里简要介绍matplotlib.pyplot模块，功能和MATLAB的作图功能类似。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h2><p>matplotlib库中最重要的函数就是plot。该函数允许你做出2D图形：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">3</span> * np.pi, <span class="number">0.1</span>)</span><br><span class="line">y = np.sin(x)</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>[ 0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.   1.1  1.2  1.3  1.4
  1.5  1.6  1.7  1.8  1.9  2.   2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9
  3.   3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.   4.1  4.2  4.3  4.4
  4.5  4.6  4.7  4.8  4.9  5.   5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9
  6.   6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.   7.1  7.2  7.3  7.4
  7.5  7.6  7.7  7.8  7.9  8.   8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9
  9.   9.1  9.2  9.3  9.4]
[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554
  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736
  0.93203909  0.96355819  0.98544973  0.99749499  0.9995736   0.99166481
  0.97384763  0.94630009  0.90929743  0.86320937  0.8084964   0.74570521
  0.67546318  0.59847214  0.51550137  0.42737988  0.33498815  0.23924933
  0.14112001  0.04158066 -0.05837414 -0.15774569 -0.2555411  -0.35078323
 -0.44252044 -0.52983614 -0.61185789 -0.68776616 -0.7568025  -0.81827711
 -0.87157577 -0.91616594 -0.95160207 -0.97753012 -0.993691   -0.99992326
 -0.99616461 -0.98245261 -0.95892427 -0.92581468 -0.88345466 -0.83226744
 -0.77276449 -0.70554033 -0.63126664 -0.55068554 -0.46460218 -0.37387666
 -0.2794155  -0.1821625  -0.0830894   0.0168139   0.1165492   0.21511999
  0.31154136  0.40484992  0.49411335  0.57843976  0.6569866   0.72896904
  0.79366786  0.85043662  0.8987081   0.93799998  0.96791967  0.98816823
  0.99854335  0.99894134  0.98935825  0.96988981  0.94073056  0.90217183
  0.85459891  0.79848711  0.7343971   0.66296923  0.58491719  0.50102086
  0.41211849  0.31909836  0.22288991  0.12445442  0.02477543]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show() <span class="comment"># 必须有该语句才能显示图形</span></span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1fw6n4ccuezj20et09wjrm.jpg" alt=""></p>
<p>只需要少量工作，就可以一次画不同的线，加上标签，坐标轴标志等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">3</span> * np.pi, <span class="number">0.1</span>)</span><br><span class="line">y_sin = np.sin(x)</span><br><span class="line">y_cos = np.cos(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y_sin)</span><br><span class="line">plt.plot(x, y_cos)</span><br><span class="line">plt.xlabel(<span class="string">"x axis label"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y axis label"</span>)</span><br><span class="line">plt.title(<span class="string">"sine and cosine"</span>)</span><br><span class="line">plt.legend([<span class="string">'sine'</span>, <span class="string">'cosine'</span>]) <span class="comment"># 图例</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1fw6n4ce6ccj20fe0avmxs.jpg" alt="png"></p>
<p>可以在<a href="https://link.zhihu.com/?target=http%3A//matplotlib.org/api/pyplot_api.html%23matplotlib.pyplot.plot" target="_blank" rel="noopener">文档</a>中阅读更多关于plot的内容。</p>
<h2 id="绘制多个图像"><a href="#绘制多个图像" class="headerlink" title="绘制多个图像"></a>绘制多个图像</h2><p>可以使用<strong>subplot</strong>函数在一幅图中画出不同的东西</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">3</span> * np.pi, <span class="number">0.1</span>)</span><br><span class="line">y_sin = np.sin(x)</span><br><span class="line">y_cos = np.cos(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up a subplot grid that has height 2 and width 1,</span></span><br><span class="line"><span class="comment"># and set the first such subplot as active.</span></span><br><span class="line"><span class="comment"># 第一个画布</span></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># (画布数(竖着放图)，画布数(横着放图) ， 第几个画布)  # 第一个画布</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个plot</span></span><br><span class="line">plt.plot(x, y_sin)</span><br><span class="line">plt.title(<span class="string">"sine"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建第二个画布，并画图</span></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>) <span class="comment"># 第二个画布</span></span><br><span class="line">plt.plot(x, y_cos)</span><br><span class="line">plt.title(<span class="string">"cosine"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建第三个画布，并画图</span></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">plt.plot(x, y_sin)</span><br><span class="line">plt.plot(x, y_cos)</span><br><span class="line">plt.title(<span class="string">"sine and cosine"</span>)</span><br><span class="line">plt.legend([<span class="string">'sine'</span>, <span class="string">'cosine'</span>])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1fw6n4cfjx4j20et0aa0tf.jpg" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up a subplot grid that has height 2 and width 1,</span></span><br><span class="line"><span class="comment"># and set the first such subplot as active.</span></span><br><span class="line"><span class="comment"># 第一个画布</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>) <span class="comment"># (画布数(竖着放图)，画布数(横着放图) ， 第几个画布)  # 第一个画布</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个plot</span></span><br><span class="line">plt.plot(x, y_sin)</span><br><span class="line">plt.title(<span class="string">"sine"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建第二个画布，并画图</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>) <span class="comment"># 第二个画布</span></span><br><span class="line">plt.plot(x, y_cos)</span><br><span class="line">plt.title(<span class="string">"cosine"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建第三个画布，并画图</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">plt.plot(x, y_sin)</span><br><span class="line">plt.plot(x, y_cos)</span><br><span class="line">plt.title(<span class="string">"sine and cosine"</span>)</span><br><span class="line">plt.legend([<span class="string">'sine'</span>, <span class="string">'cosine'</span>])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1fw6n4cee1pj20et0aa74u.jpg" alt="png"></p>
<p>关于subplot的更多细节，可以阅读<a href="https://link.zhihu.com/?target=http%3A//matplotlib.org/api/pyplot_api.html%23matplotlib.pyplot.plot" target="_blank" rel="noopener">文档</a>。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/13/一文入门numpy/">一文入门numpy</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-13</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/numpy/">numpy</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/python/">python</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/numpy/">numpy</a></span><div class="content"><p>Numpy是python中用于科学计算的核心库。它提供了高性能的多维数组对象，记忆相关工具。</p>
<p>本文是一片入门文章，带你立马进入科学计算的世界。之后，再继续详细的学习Numpy</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<h2 id="创建ndarray——Numpy数组对象Arrays"><a href="#创建ndarray——Numpy数组对象Arrays" class="headerlink" title="创建ndarray——Numpy数组对象Arrays"></a>创建ndarray——Numpy数组对象Arrays</h2><p>Numpy中的多维数组称为ndarray，这是Numpy中最常见的数组对象。ndarray对象通常包含两个部分：</p>
<ul>
<li>ndarray数据本身</li>
<li>描述数据的元数据</li>
</ul>
<p>一个numpy数组是一个由不同数值组成的网格。网格中的数据都是统一数据类型，可以通过非负整型数的元组来访问。维度的数量被称为数组的阶，数组的大小是一个由整型数构成的元组，可以描述数组不同维度上的大小。</p>
<h3 id="创建ndarray的方法有很多种："><a href="#创建ndarray的方法有很多种：" class="headerlink" title="创建ndarray的方法有很多种："></a>创建ndarray的方法有很多种：</h3><h4 id="Method1-基于list或者tuple"><a href="#Method1-基于list或者tuple" class="headerlink" title="Method1: 基于list或者tuple"></a>Method1: 基于list或者tuple</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于list</span></span><br><span class="line"><span class="comment">#一维数组</span></span><br><span class="line">arr1_list = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">print(arr1_list)</span><br><span class="line">print(arr1_list.shape) <span class="comment"># 查看维数</span></span><br><span class="line"><span class="comment">#二维数组(2*3)</span></span><br><span class="line">arr2_list = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>], [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">print(arr2_list)</span><br><span class="line">print(arr2_list.shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"*****************"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#基于tuple</span></span><br><span class="line"><span class="comment"># 一维数组</span></span><br><span class="line">arr1_tuple = np.array((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">print(arr1_tuple)</span><br><span class="line"><span class="comment"># 二维数组</span></span><br><span class="line">arr2_tuple = np.array(((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)))</span><br><span class="line">print(arr2_tuple)</span><br></pre></td></tr></table></figure>
<pre><code>[1 2 3 4]
(4,)
[[1 2 4]
 [3 4 5]]
(2, 3)
*****************
[1 2 3 4]
[[1 2 3]
 [3 4 5]]
</code></pre><h4 id="Method2-基于np-arange"><a href="#Method2-基于np-arange" class="headerlink" title="Method2: 基于np.arange"></a>Method2: 基于np.arange</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#一维数据</span></span><br><span class="line">arr1 = np.arange(<span class="number">5</span>)</span><br><span class="line">print(arr1)</span><br></pre></td></tr></table></figure>
<pre><code>[0 1 2 3 4]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#二维数组</span></span><br><span class="line">arr2 = np.array([np.arange(<span class="number">3</span>), np.arange(<span class="number">3</span>)])</span><br><span class="line">print(arr2)</span><br></pre></td></tr></table></figure>
<pre><code>[[0 1 2]
 [0 1 2]]
</code></pre><h4 id="Method3-基于arange以及reshape创建多维数组"><a href="#Method3-基于arange以及reshape创建多维数组" class="headerlink" title="Method3: 基于arange以及reshape创建多维数组"></a>Method3: 基于arange以及reshape创建多维数组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建三维数组</span></span><br><span class="line">arr3 = np.arange(<span class="number">24</span>)</span><br><span class="line">print(arr3)</span><br><span class="line">print(<span class="string">"********************"</span>)</span><br><span class="line">arr3 = np.arange(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>) </span><br><span class="line"><span class="comment">#(a, b, c) a*b*c=24 a:量的维数 b:单矩阵的行数 c:单矩阵的列数</span></span><br><span class="line">print(arr3)</span><br><span class="line">print(<span class="string">"********************"</span>)</span><br><span class="line">arr3 = np.arange(<span class="number">24</span>).reshape(<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">print(arr3)</span><br></pre></td></tr></table></figure>
<pre><code>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
********************
[[[ 0  1  2  3]
  [ 4  5  6  7]
  [ 8  9 10 11]]

 [[12 13 14 15]
  [16 17 18 19]
  [20 21 22 23]]]
********************
[[[ 0  1]
  [ 2  3]
  [ 4  5]
  [ 6  7]]

 [[ 8  9]
  [10 11]
  [12 13]
  [14 15]]

 [[16 17]
  [18 19]
  [20 21]
  [22 23]]]
</code></pre><p>请注意：arange的长度与ndarray的维度的乘积要相等，即 24 = 2X3X4</p>
<h4 id="Methonx-其他方法"><a href="#Methonx-其他方法" class="headerlink" title="Methonx: 其他方法"></a>Methonx: 其他方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.zeros((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">print(a)</span><br><span class="line">print(a.shape)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.  0.]
 [ 0.  0.]]
(2, 2)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = np.ones((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">print(b)</span><br><span class="line">print(b.shape)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 1.  1.]]
(1, 2)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = np.full((<span class="number">2</span>, <span class="number">2</span>), <span class="number">7</span>) <span class="comment"># 常量矩阵</span></span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 7.  7.]
 [ 7.  7.]]


/storage/st13/tool/a1/lib/python3.5/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((2, 2), 7) will return an array of dtype(&apos;int64&apos;)
  format(shape, fill_value, array(fill_value).dtype), FutureWarning)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d = np.eye(<span class="number">2</span>) <span class="comment"># Create a 2x2 identity matrix</span></span><br><span class="line">print(d)</span><br><span class="line">print(<span class="string">"*************"</span>)</span><br><span class="line">d = np.eye(<span class="number">5</span>)</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 1.  0.]
 [ 0.  1.]]
*************
[[ 1.  0.  0.  0.  0.]
 [ 0.  1.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.]
 [ 0.  0.  0.  1.  0.]
 [ 0.  0.  0.  0.  1.]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">e = np.random.random((<span class="number">2</span>,<span class="number">2</span>)) <span class="comment"># 随机矩阵</span></span><br><span class="line">print(e)</span><br><span class="line">print(<span class="string">"***********************"</span>)</span><br><span class="line">e = np.random.random((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">print(e)</span><br><span class="line">print(<span class="string">"***********************"</span>)</span><br><span class="line">e = np.random.random((<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line">print(e)</span><br><span class="line">print(<span class="string">"***********************"</span>)</span><br><span class="line">e = np.random.random((<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line">print(e)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.85433662  0.39258359]
 [ 0.29631359  0.05891357]]
***********************
[[ 0.64301123  0.99398848  0.76786152]
 [ 0.48457956  0.78617157  0.59321882]
 [ 0.87496542  0.63363914  0.29266131]]
***********************
[[ 0.95488923  0.48188791  0.69978935  0.45231896  0.17893613]
 [ 0.14353007  0.30440851  0.99860637  0.30771004  0.07808516]
 [ 0.25500584  0.90187196  0.62752036  0.87251994  0.0885015 ]]
***********************
[[[ 0.25445793  0.29760547  0.01109148  0.88356824  0.69800616]
  [ 0.20794927  0.84670214  0.68143358  0.15878092  0.4199746 ]
  [ 0.00471408  0.00620308  0.0061098   0.94018075  0.33162902]]

 [[ 0.46792911  0.4289224   0.80178591  0.59352602  0.77850049]
  [ 0.24100767  0.54646633  0.29545157  0.47070267  0.10661919]
  [ 0.3629408   0.77001824  0.00791864  0.25837697  0.20175684]]]
</code></pre><p>其他数组的相关方法，<a href="https://docs.scipy.org/doc/numpy/user/basics.creation.html" target="_blank" rel="noopener">文档</a></p>
<h2 id="访问数组"><a href="#访问数组" class="headerlink" title="访问数组"></a>访问数组</h2><h3 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h3><p>和python列表类似，numpy数组可以使用切片语法。因为数组可以是多维的，所有<strong>必须为每个维度指定好切片</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.Create the following rank 2 array with shape (3, 4)</span></span><br><span class="line"><span class="comment"># [[ 1  2  3  4]</span></span><br><span class="line"><span class="comment">#  [ 5  6  7  8]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11 12]]</span></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.Use slicing to pull out the subarray consisting of the first 2 rows</span></span><br><span class="line"><span class="comment"># and columns 1 and 2; b is the following array of shape (2, 2):</span></span><br><span class="line"><span class="comment"># [[2 3]</span></span><br><span class="line"><span class="comment">#  [6 7]]</span></span><br><span class="line">b = a[:<span class="number">2</span>, <span class="number">1</span>:<span class="number">3</span>] <span class="comment"># [行: 前X行， 行slice: a:b]</span></span><br><span class="line">print(b)</span><br><span class="line">b = a[:<span class="number">3</span>, :<span class="number">3</span>]</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<pre><code>[[2 3]
 [6 7]]
[[ 1  2  3]
 [ 5  6  7]
 [ 9 10 11]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.A slice of an array is a view into the same data, so modifying it</span></span><br><span class="line"><span class="comment"># will modify the original array.</span></span><br><span class="line">print(a[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">a[<span class="number">0</span>, <span class="number">1</span>] = <span class="number">520</span></span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<pre><code>2
[[  1 520   3   4]
 [  5   6   7   8]
 [  9  10  11  12]]
</code></pre><h3 id="整型数组访问"><a href="#整型数组访问" class="headerlink" title="整型数组访问"></a>整型数组访问</h3><p>当我们使用切片语法访问数组时，得到的总是原数组的一个子集。整型数组访问允许我们<strong>利用其它数组的数据构建一个新的数组</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">print(a)</span><br><span class="line">print(a.shape)</span><br></pre></td></tr></table></figure>
<pre><code>[[1 2]
 [3 4]
 [5 6]]
(3, 2)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(a[[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]]) <span class="comment">#a[[第几行，][第几列，]]</span></span><br><span class="line">print(a[[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line">print(a[[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>[5 5 5 5]
[5 5 5 4]
[5 5 6]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(np.array([a[<span class="number">0</span>, <span class="number">0</span>], a[<span class="number">1</span>, <span class="number">1</span>], a[<span class="number">2</span>, <span class="number">0</span>]]))</span><br></pre></td></tr></table></figure>
<pre><code>[1 4 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(a[[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>]]) <span class="comment">#a[[第几行，][第几列，]]</span></span><br></pre></td></tr></table></figure>
<pre><code>[2 4]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(np.array([a[<span class="number">0</span>, <span class="number">1</span>], a[<span class="number">0</span>, <span class="number">1</span>]])) <span class="comment">#a[[第几行，][第几列，]]</span></span><br></pre></td></tr></table></figure>
<pre><code>[2 2]
</code></pre><p>整型数组访问语法还有个有用的技巧，可以用来选择或者更改矩阵中每行中的一个元素：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.Create a new array from which we will select elements</span></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 1  2  3]
 [ 4  5  6]
 [ 7  8  9]
 [10 11 12]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.Create an array of indices array的index</span></span><br><span class="line">b = np.array([<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<pre><code>[0 2 0 1]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.Select one element from each row of a using the indices in b</span></span><br><span class="line">print(a[np.arange(<span class="number">4</span>), b])</span><br></pre></td></tr></table></figure>
<pre><code>[ 1  6  7 11]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.Mutate one element from each row of a using the indices in b</span></span><br><span class="line">a[np.arange(<span class="number">4</span>), b] += <span class="number">10</span></span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<pre><code>[[21  2  3]
 [ 4  5 26]
 [27  8  9]
 [10 31 12]]
</code></pre><h3 id="布尔型数组访问"><a href="#布尔型数组访问" class="headerlink" title="布尔型数组访问"></a>布尔型数组访问</h3><p>布尔型数组访问可以让你选择数组中任意元素。通常，这种访问方式用于<strong>选取数组中满足某些条件的元素</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<pre><code>[[1 2]
 [3 4]
 [5 6]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bool_idx = (a &gt; <span class="number">2</span>)</span><br><span class="line">print(bool_idx)</span><br></pre></td></tr></table></figure>
<pre><code>[[False False]
 [ True  True]
 [ True  True]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(a[bool_idx])</span><br><span class="line">print(a[a &gt; <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[3 4 5 6]
[3 4 5 6]
</code></pre><p>为了教程的简介，有很多数组访问的细节我们没有详细说明，可以查看<a href="https://docs.scipy.org/doc/numpy/user/basics.creation.html" target="_blank" rel="noopener">文档</a></p>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>Numpy的数值类型如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1fw6d6gxtndj20go0i2424.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">print(x.dtype)</span><br></pre></td></tr></table></figure>
<pre><code>int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">print(x.dtype)</span><br></pre></td></tr></table></figure>
<pre><code>float64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>], dtype=np.int64)</span><br><span class="line">print(x.dtype)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>int64
[1 2]
</code></pre><p>更多细节查看<a href="https://docs.scipy.org/doc/numpy/user/basics.creation.html" target="_blank" rel="noopener">文档</a></p>
<h2 id="数组计算"><a href="#数组计算" class="headerlink" title="数组计算"></a>数组计算</h2><p>基本数学计算函数，会对数组中<strong>元素逐个进行计算</strong>，既可以利用操作符重载，也可以使用函数方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]], dtype=np.float64)</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 1.  2.]
 [ 3.  4.]]
[[ 5.  6.]
 [ 7.  8.]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># "矩阵"相加——元素相加</span></span><br><span class="line">print(x + y)</span><br><span class="line">print(<span class="string">"**************"</span>)</span><br><span class="line">print(np.add(x, y))</span><br></pre></td></tr></table></figure>
<pre><code>[[  6.   8.]
 [ 10.  12.]]
[[  6.   8.]
 [ 10.  12.]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># "矩阵"相减——元素相减</span></span><br><span class="line">print(x - y)</span><br><span class="line">print(<span class="string">"**************"</span>)</span><br><span class="line">print(np.subtract(x, y))</span><br></pre></td></tr></table></figure>
<pre><code>[[-4. -4.]
 [-4. -4.]]
[[-4. -4.]
 [-4. -4.]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># "矩阵"乘法——元素相乘</span></span><br><span class="line">print(x * y)</span><br><span class="line">print(<span class="string">"*************"</span>)</span><br><span class="line">print(np.multiply(x, y))</span><br></pre></td></tr></table></figure>
<pre><code>[[  5.  12.]
 [ 21.  32.]]
*************
[[  5.  12.]
 [ 21.  32.]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># "矩阵"除法——元素相除</span></span><br><span class="line">print(x / y)</span><br><span class="line">print(<span class="string">"************"</span>)</span><br><span class="line">print(np.divide(x, y))</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.2         0.33333333]
 [ 0.42857143  0.5       ]]
************
[[ 0.2         0.33333333]
 [ 0.42857143  0.5       ]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 平方根"矩阵"——元素开根</span></span><br><span class="line">print(np.sqrt(x))</span><br></pre></td></tr></table></figure>
<pre><code>[[ 1.          1.41421356]
 [ 1.73205081  2.        ]]
</code></pre><p>和MATLAB不同，<strong>*是元素逐个相乘，而不是矩阵乘法</strong>。</p>
<p><strong>在Numpy中使用dot来进行矩阵乘法：</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1fw6ebckje0j20ay01ot8h.jpg" alt=""></p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1fw6ehktoogj20gi01lmwz.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">y = np.array([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>[[1 2]
 [3 4]]
[[5 6]
 [7 8]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = np.array([<span class="number">9</span>,<span class="number">10</span>])</span><br><span class="line">w = np.array([<span class="number">11</span>, <span class="number">12</span>])</span><br><span class="line">print(v)</span><br><span class="line">print(w)</span><br></pre></td></tr></table></figure>
<pre><code>[ 9 10]
[11 12]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(v.dot(w))</span><br><span class="line">print(np.dot(v, w))</span><br></pre></td></tr></table></figure>
<pre><code>219
219
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(x.dot(v))</span><br><span class="line">print(np.dot(x, v))</span><br></pre></td></tr></table></figure>
<pre><code>[29 67]
[29 67]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(x.dot(y))</span><br><span class="line">print(np.dot(x, y))</span><br></pre></td></tr></table></figure>
<pre><code>[[19 22]
 [43 50]]
[[19 22]
 [43 50]]
</code></pre><p>Numpy提供了很多计算数组的函数，其中最常用的一个是sum(元素之和)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>[[1 2]
 [3 4]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(np.sum(x))</span><br><span class="line">print(np.sum(x, axis=<span class="number">0</span>)) <span class="comment"># 列之和</span></span><br><span class="line">print(np.sum(x, axis=<span class="number">1</span>)) <span class="comment"># 行之和</span></span><br></pre></td></tr></table></figure>
<pre><code>10
[4 6]
[3 7]
</code></pre><p>想要了解更多函数，可以查看<a href="https://docs.scipy.org/doc/numpy/user/basics.creation.html" target="_blank" rel="noopener">文档</a></p>
<p>除了计算，我们还常常改变数组或者操作其中的元素。其中将矩阵转置是常用的一个，在Numpy中，使用T来转置矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>[[1 2]
 [3 4]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.T)</span><br></pre></td></tr></table></figure>
<pre><code>[[1 3]
 [2 4]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note that taking the transpose of a rank 1 array does nothing:</span></span><br><span class="line">v = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">print(v)</span><br></pre></td></tr></table></figure>
<pre><code>[1 2 3]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(v.T)</span><br></pre></td></tr></table></figure>
<pre><code>[1 2 3]
</code></pre><p>Numpy还提供了更多操作数组的方法，请查看<a href="https://docs.scipy.org/doc/numpy/user/basics.creation.html" target="_blank" rel="noopener">文档</a></p>
<h2 id="广播机制-Broadcasting"><a href="#广播机制-Broadcasting" class="headerlink" title="广播机制 Broadcasting"></a>广播机制 Broadcasting</h2><p>广播是一种强有力的机制，它让Numpy可以让<strong>不同大小的矩阵在一起进行数学计算</strong>。我们常常会有一个小的矩阵和一个大的矩阵，然后我们会需要用小的矩阵对大的矩阵做一些计算。</p>
<p>举个例子，如果我们想要把一个向量加到矩阵的每一行，我们可以这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">v = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y = np.empty_like(x)   <span class="comment"># Create an empty matrix with the same shape as x</span></span><br><span class="line">print(x)</span><br><span class="line">print(v)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 1  2  3]
 [ 4  5  6]
 [ 7  8  9]
 [10 11 12]]
[1 0 1]
[[0 0 0]
 [0 0 0]
 [0 0 0]
 [0 0 0]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x[<span class="number">1</span> : <span class="number">2</span>, <span class="number">0</span> : <span class="number">1</span>]) <span class="comment"># x[行 ，列]</span></span><br></pre></td></tr></table></figure>
<pre><code>[[4]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(x[<span class="number">0</span>, :]) <span class="comment"># 取第0行</span></span><br><span class="line">print(x[<span class="number">0</span>, : <span class="number">2</span>,]) <span class="comment"># 取第0行 前2个</span></span><br></pre></td></tr></table></figure>
<pre><code>[1 2 3]
[1 2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    y[i, :] = x[i, :] + v</span><br><span class="line">    </span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 2  2  4]
 [ 5  5  7]
 [ 8  8 10]
 [11 11 13]]
</code></pre><p>这样是行得通的，但是当x矩阵非常大，利用循环来计算就会变得很慢很慢。我们可以换一种思路：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">v = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">vv = np.tile(v, (<span class="number">4</span>, <span class="number">1</span>))  <span class="comment"># Stack 4 copies of v on top of each other</span></span><br><span class="line">print( vv  )</span><br></pre></td></tr></table></figure>
<pre><code>[[1 0 1]
 [1 0 1]
 [1 0 1]
 [1 0 1]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + vv</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 2  2  4]
 [ 5  5  7]
 [ 8  8 10]
 [11 11 13]]
</code></pre><p>Numpy广播机制可以让我们不用创建vv，就能直接运算，看看下面例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">v = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y = x + v</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 2  2  4]
 [ 5  5  7]
 [ 8  8 10]
 [11 11 13]]
</code></pre><p>对两个数组使用广播机制要遵守下列规则：</p>
<ul>
<li>如果数组的秩不同，使用1来将秩较小的数组进行扩展，直到两个数组的尺寸的长度都一样。</li>
<li>如果两个数组在某个维度上的长度是一样的，或者其中一个数组在该维度上长度为1，那么我们就说这两个数组在该维度上是相容的。</li>
<li>如果两个数组在所有维度上都是相容的，他们就能使用广播。</li>
<li>如果两个输入数组的尺寸不同，那么注意其中较大的那个尺寸。因为广播之后，两个数组的尺寸将和那个较大的尺寸一样。</li>
<li>在任何一个维度上，如果一个数组的长度为1，另一个数组长度大于1，那么在该维度上，就好像是对第一个数组进行了复制。</li>
</ul>
<p>如果上述解释看不明白，可以读一读<a href="https://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/user/basics.broadcasting.html" target="_blank" rel="noopener">文档</a>和这个<a href="https://link.zhihu.com/?target=http%3A//scipy.github.io/old-wiki/pages/EricsBroadcastingDoc" target="_blank" rel="noopener">解释</a>。译者注：强烈推荐阅读文档中的例子。</p>
<p>支持广播机制的函数是全局函数。哪些是全局函数可以在<a href="https://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/ufuncs.html%23available-ufuncs" target="_blank" rel="noopener">文档</a>中查找。</p>
<p>下面是一些广播机制的使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.</span></span><br><span class="line">v = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])  <span class="comment"># v has shape (3,)</span></span><br><span class="line">w = np.array([<span class="number">4</span>,<span class="number">5</span>])    <span class="comment"># w has shape (2,)</span></span><br><span class="line">print(v)</span><br><span class="line">print(w)</span><br></pre></td></tr></table></figure>
<pre><code>[1 2 3]
[4 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(np.reshape(v, (<span class="number">3</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>[[1]
 [2]
 [3]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(np.reshape(v, (<span class="number">3</span>, <span class="number">1</span>))* w)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 4  5]
 [ 8 10]
 [12 15]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. # x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),</span></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<pre><code>[[1 2 3]
 [4 5 6]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x + v)</span><br></pre></td></tr></table></figure>
<pre><code>[[2 4 6]
 [5 7 9]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.# Add a vector to each column of a matrix</span></span><br><span class="line"><span class="comment"># x has shape (2, 3) and w has shape (2,).</span></span><br><span class="line"><span class="comment"># If we transpose x then it has shape (3, 2) and can be broadcast</span></span><br><span class="line"><span class="comment"># against w to yield a result of shape (3, 2); transposing this result</span></span><br><span class="line"><span class="comment"># yields the final result of shape (2, 3) which is the matrix x with</span></span><br><span class="line"><span class="comment"># the vector w added to each column. Gives the following matrix:</span></span><br><span class="line"><span class="comment"># [[ 5  6  7]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11]]</span></span><br><span class="line">print(x.T)</span><br><span class="line">print(x.T + w)</span><br><span class="line">print((x.T + w).T)</span><br></pre></td></tr></table></figure>
<pre><code>[[1 4]
 [2 5]
 [3 6]]
[[ 5  9]
 [ 6 10]
 [ 7 11]]
[[ 5  6  7]
 [ 9 10 11]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.Another solution is to reshape w to be a row vector of shape (2, 1);</span></span><br><span class="line"><span class="comment"># we can then broadcast it directly against x to produce the same</span></span><br><span class="line"><span class="comment"># output.</span></span><br><span class="line">print( x + np.reshape(w, (<span class="number">2</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>[[ 5  6  7]
 [ 9 10 11]]
</code></pre><p><strong>广播机制能够让你的代码更简洁更迅速，能够用的时候请尽量使用！</strong></p>
<h2 id="Numpy文档"><a href="#Numpy文档" class="headerlink" title="Numpy文档"></a>Numpy文档</h2><p>这篇教程涉及了你需要了解的numpy中的一些重要内容，但是numpy远不止如此。可以查阅<a href="https://docs.scipy.org/doc/numpy/user/basics.creation.html" target="_blank" rel="noopener">numpy文档</a>来学习更多。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/07/06/记一次爱奇艺多模态比赛过程/">记一次爱奇艺多模态比赛过程</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-07-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/参加比赛/">参加比赛</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/比赛/">比赛</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/记录/">记录</a></span><div class="content"><!-- # 爱奇艺多模态视频分析大赛 -->
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>开这一篇博客是想记录一下参加比赛的一个过程，也是第一次参加深度学习的比赛，作为一个好的开端吧。</p>
<h1 id="2018年7月6日"><a href="#2018年7月6日" class="headerlink" title="2018年7月6日"></a>2018年7月6日</h1><p>准备对数据集下手了，目前爱奇艺的数据集只是以<code>xxxxxxx.mp4 ID</code>的形式给出，所以我们需要对数据集进行预处理，最后得到了一个ID文件夹名对应一些视频的形式，如下图：</p>
<div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ft1m8v61dfj20hb0ekab1.jpg" alt="明星ID"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ft1m9r1u45j20el064dgl.jpg" alt="一个ID对应不同的"><br><br></div>

<p>这种形式方便进行多目标分类。</p>
<p>目前打算采用的方法：①将视频做成截图的形式②由于视频较少，采用迁移学习的方法</p>
<h1 id="2018年7月7日"><a href="#2018年7月7日" class="headerlink" title="2018年7月7日"></a>2018年7月7日</h1><p>任务：</p>
<ul>
<li>采用截图的形式，将每个ID对应的视频截取成图片的形式</li>
</ul>
<p>完成：效果如图所示</p>
<div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ft2qs4l3czj21ct0pt7ts.jpg" alt="ID对应视频截图"><br><br></div>

<p>正在将文件夹上传到集群当作，希望明天能做迁移学习。</p>
<h1 id="2018年7月8日-7月9日"><a href="#2018年7月8日-7月9日" class="headerlink" title="2018年7月8日-7月9日"></a>2018年7月8日-7月9日</h1><p>这两天才把1hz的截图文件上传到集群当中，但是考虑到1hz的数据量实在是太大，达到40个G，这还只是part1数据，如果再加上part2的数据的话，情况将不可想象。</p>
<div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ft4j2y055pj21cf0lsb29.jpg" alt=""><br><br></div>

<p>所以我们有将视频以5hz的形式截取视频的截图，以减小数据的大小。</p>
<div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ft4j3goeqqj21cm0ocnhd.jpg" alt=""><br><br></div>

<p>接下来我们将对数据进行可视化操作，以对数据有一个更直观的感受。</p>
<h2 id="对ID2VIDEO进行数据可视化："><a href="#对ID2VIDEO进行数据可视化：" class="headerlink" title="对ID2VIDEO进行数据可视化："></a>对ID2VIDEO进行数据可视化：</h2><div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ft4j4x23uzj21uo0yr764.jpg" alt=""><br><br></div>

<p>我们可以看到：每个ID对应许多的视频文件，一个ID最多对应的视频数为69，最低为1。<br>我们可能不会采用视频的形式进行神经网络的学习。所有这个可视化，我们仅供参考。看看就行。</p>
<h2 id="对ID2IMG-1hz进行数据可视化："><a href="#对ID2IMG-1hz进行数据可视化：" class="headerlink" title="对ID2IMG_1hz进行数据可视化："></a>对ID2IMG_1hz进行数据可视化：</h2><p>对ID2IMG_1hz(1帧的频率截取的图片)进行数据可视化：</p>
<div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ft4j58f12ej21uo0yr40h.jpg" alt=""><br><br></div>

<p>我们可以看到：数据量实在是太多，平均每个ID文件有800左右的图片，最高数据为7000左右，最低为50左右。这还只是1G的视频文件呢。瑟瑟发抖。</p>
<h2 id="对ID2IMG-5hz进行数据可视化："><a href="#对ID2IMG-5hz进行数据可视化：" class="headerlink" title="对ID2IMG_5hz进行数据可视化："></a>对ID2IMG_5hz进行数据可视化：</h2><p>于是，我们考虑采用5hz的形式进行截图，可视化：</p>
<div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ft4j5f2c08j21uo0yrmz3.jpg" alt=""><br><br></div>

<h2 id="两个可视化合并："><a href="#两个可视化合并：" class="headerlink" title="两个可视化合并："></a>两个可视化合并：</h2><div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ft4j5lkwy1j21uo0yr40w.jpg" alt=""><br><br></div>

<p>我们可以看到：与1hz的相比，数据的分布还是具有一致性的（废话！），但是数据的大小大大降低，但是每个ID对应的图片数量也大大降低了，不知道对网络学习是否有影响。若有影响或将采取以下方案：</p>
<ul>
<li>数据增强</li>
<li>迁移学习</li>
</ul>
<h1 id="2018年7月10日-8月1日"><a href="#2018年7月10日-8月1日" class="headerlink" title="2018年7月10日-8月1日"></a>2018年7月10日-8月1日</h1><p>这段时间一直在忙着训练网络，修改参数。没有时间更新，好在能得到一个比较好的结果：</p>
<h2 id="网络训练-7月10日-7月20日"><a href="#网络训练-7月10日-7月20日" class="headerlink" title="网络训练 7月10日-7月20日"></a>网络训练 7月10日-7月20日</h2><h3 id="第一次训练"><a href="#第一次训练" class="headerlink" title="第一次训练"></a>第一次训练</h3><ul>
<li>数据集：5hz训练集图片</li>
<li>网络结构：Resnet50（pre-trained）</li>
<li>模式：迁移学习 fine-tune</li>
<li>epoch次数：20</li>
</ul>
<p>我们用Resnet50的网络结构进行迁移学习，学习我们所要的图片特征：</p>
<div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvmb18w61j20pp09hwi4.jpg" alt=""><br><br></div>

<p>我们可以看到我们的数据在训练集上的准确度是比较好的，最后可以达到99.1%。但是在验证集上的表现不是很好，虽然也可以达到74.8%，但是要达到我们所需要的准确度还是有一定的差距的。</p>
<h3 id="第一次训练之后的数据预处理"><a href="#第一次训练之后的数据预处理" class="headerlink" title="第一次训练之后的数据预处理"></a>第一次训练之后的数据预处理</h3><p>我们考虑是数据集的问题，于是我们对数据集进行处理，采用MTCNN对数据中的脸部数据进行保留，只保留人脸：</p>
<p>数据处理前：</p>
<div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvmlg32qgj20ow0e0myt.jpg" alt=""><br><br></div>

<p>MTCNN处理之后：</p>
<div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvmlg2p5tj202o034742.jpg" alt=""><br><br></div>

<p>我们将part1中所有的数据进行“扣脸”处理。</p>
<h3 id="第二次网络训练"><a href="#第二次网络训练" class="headerlink" title="第二次网络训练"></a>第二次网络训练</h3><ul>
<li>数据集：5hz训练集图片 <strong>进行了“扣脸”处理</strong></li>
<li>网络结构：Resnet50（pre-trained）</li>
<li>模式：迁移学习 fine-tune</li>
<li>epoch次数：20</li>
</ul>
<p>结果如下：</p>
<div align="center"><br><br><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvmqenpgvj20lm09jwhe.jpg" alt=""><br><br></div>

<p>我们可以看到：我们在训练集上的准确度可以达到96%，但是在验证集上的准确率只有区区的2.1%。我们怀疑是不是不应该进行“扣脸”处理，所有我们放弃这一次的训练。<strong>保存了第一次训练的模型和参数</strong></p>
<h2 id="图像检索-7月21日-7月30"><a href="#图像检索-7月21日-7月30" class="headerlink" title="图像检索 7月21日-7月30"></a>图像检索 7月21日-7月30</h2><p>我们将第一次训练好的模型以及参数保存下来，进行图像检索。</p>
<p>图像检索我们采用“余弦相似度”的方法进行比较两个图片的显示度（之后会写一篇文章详细的介绍<em>余弦相似度</em>）。</p>
<p>主要方案如下：</p>
<ul>
<li>1.将需要检索的图片载入训练好的模型当中。</li>
<li>2.模型会输出这个图片的向量（tensor）形式。</li>
<li>3.我们将检索池中的待检索图片进行12步，得到待检索图片的向量（tensor）形式。</li>
<li>4.利用余弦相似度进行两个图片的对比，将相似度高的图片保留，最后得到检索结果。</li>
</ul>
<p>我们采用每一个ID的第</p>
<p>结果如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvniy7ic5j20ty0mrwhp.jpg" alt=""></p>
<p>我们检测ID1：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvnnedx5pj20ep05iaa4.jpg" alt=""></p>
<p>我们能完全的检测到！！！很棒！</p>
<p>我们再随机抽取ID20：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvnopr4amj20fn04x0sq.jpg" alt=""></p>
<p>我们可以打开预测错误的视频：</p>
<p>0270：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvntirhgkj20nk0dctaq.jpg" alt=""></p>
<p>0827：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvntirlacj20nk0d4wgr.jpg" alt=""></p>
<p>我们看到相似度并不是很高，我们查看这两个图片的余弦相似度：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvo42i0nqj206d01idfn.jpg" alt=""></p>
<p>只有0.6604，可以看到相似度并不是很高，我们再看看，原始图片：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvo79xdklj20nk0dcgnc.jpg" alt=""></p>
<p>我们将原图和0270计算相似度得到：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvo79v3zcj206x01o3yb.jpg" alt=""></p>
<p>再将原图与0827计算相似度得到：</p>
<p><img src="http://ww1.sinaimg.cn/large/73c0c3d4gy1ftvo7a5kwcj206101idfn.jpg" alt=""></p>
<p>于是我们可以将阈值提高：<strong>只有相似度达到90%以上</strong>才能判断为同一人物（ID）</p>
<p>同时，我们再查看其它图片时发现，同一视频来源的图片（视频）更容易被找到。</p>
<p><strong>解决方案：我们将ID下的每个视频都截取一帧关键帧（特征最明显的）作为检索图片，这样即使提高相似度阈值，我们也能准确的找到相同的人（目前的想法）</strong></p>
<p>同时，我们也要重新对数据集进行处理，保证数据集的干净程度满足要求。</p>
<p>接下来的任务：从数据集预处理开始，重新完成一次。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/03/12/Linux中GPU使用情况/">查看Linux中GPU使用情况</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-03-12</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/环境配置/">环境配置</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/linux/">linux</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/GPU/">GPU</a></span><div class="content"><h2 id="查看静态使用情况"><a href="#查看静态使用情况" class="headerlink" title="查看静态使用情况"></a>查看静态使用情况</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvidia-smi</span><br></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/10306508-75bba159ef3cadab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="查看动态使用情况"><a href="#查看动态使用情况" class="headerlink" title="查看动态使用情况"></a>查看动态使用情况</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ watch nvidia-smi</span><br><span class="line"># 每两秒刷新</span><br></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/10306508-879d305affa21749.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ watch -n * nvidia-smi</span><br><span class="line"># 每*秒刷新</span><br><span class="line">$ watch -n 10 nvidia-smi</span><br><span class="line"># 每10秒刷新</span><br></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/10306508-6e5ce4eadf728c3d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h2><p><img src="http://upload-images.jianshu.io/upload_images/10306508-839559f02b0c8c6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
</div><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By JunDa Ren (Richard Ren)</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.0"></script><script src="/js/fancybox.js?version=1.6.0"></script><script src="/js/sidebar.js?version=1.6.0"></script><script src="/js/copy.js?version=1.6.0"></script><script src="/js/fireworks.js?version=1.6.0"></script><script src="/js/transition.js?version=1.6.0"></script><script src="/js/scroll.js?version=1.6.0"></script><script src="/js/head.js?version=1.6.0"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>